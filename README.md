<h1 align='center'>
         üîç Analisador L√©xico-Sint√°tico
</h1>

<p align="center">
  <img src="https://img.shields.io/badge/C%2B%2B-00599C?style=for-the-badge&logo=c%2B%2B&logoColor=white" alt="C++">
  <img src="https://img.shields.io/badge/GCC-orange?style=for-the-badge&logo=gnu-compiler-collection&logoColor=white" alt="GCC">
  <img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="Python">
  <img src="https://img.shields.io/badge/Ubuntu-E95420?style=for-the-badge&logo=ubuntu&logoColor=white" alt="Ubuntu">
  <img src="https://img.shields.io/badge/Visual_Studio_Code-007ACC?style=for-the-badge&logo=visual-studio-code&logoColor=white" alt="Visual Studio Code">
  <img src="https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white" alt="Git">
  <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub">
  
<img src="https://img.shields.io/badge/-Makefile-red?style=for-the-badge&logo=gnu-make&logoColor=white" alt="Makefile">
<img src="https://img.shields.io/badge/doxygen-2C4AA8?style=for-the-badge&logo=doxygen&logoColor=white" alt="Doxygen"></p>
<div align='center'>
Laborat√≥rio de Algoritmos e Estruturas de Dados I <br>
Engenharia de Computa√ß√£o <br>
Prof. Michel Pires da Silva <br>
CEFET-MG Campus V <br>
2026/1  
</div>

<details>
<summary><h2>üìã √çndice</h2></summary>
</details>

## üìù Introdu√ß√£o

Este projeto foi desenvolvido como trabalho de aproveitamento da disciplina de Laborat√≥rio de Algor√≠tmos e Estruturas de Dados I (LAEDI), sob a orienta√ß√£o do professor [Michel Pires Silva](https://github.com/mpiress). Este trabalho tem como objetivo principal o desenvolvimento de um sistema denominado Analisador L√©xico-Sint√°tico (LSA), capaz de avaliar diferentes m√©tricas associadas √† qualidade textual.
O LSA √© respons√°vel por processar um texto de entrada e extrair informa√ß√µes estat√≠sticas, estruturais e sem√¢nticas sobre seu conte√∫do. A partir da leitura sequencial do texto, o sistema identifica palavras (*tokens*), express√µes compostas, senten√ßas, par√°grafos e s√≠mbolos de pontua√ß√£o, organizando esses dados por meio de estruturas de dados implementadas manualmente, como:
* Listas encadeadas (LinkedList).
* Pilhas (Stack).
* Filas (Queue).
* Tabelas hash (HashTable).
Todas as estruturas foram implementadas durante o projeto, sem o uso de bibliotecas prontas da Standard Template Library (STL), respeitando as restri√ß√µes e objetivos did√°ticos da disciplina aproveitada.
Durante o processamento do texto, o sistema realiza diversas tarefas, entre elas:
* Contagem da frequ√™ncia de palavras e express√µes.
* Identifica√ß√£o de palavras irrelevantes (*stop words*).
* Registro detalhado das ocorr√™ncias dos *tokens* (par√°grafo, senten√ßa, linha e posi√ß√£o).
* Verifica√ß√£o do balanceamento de s√≠mbolos de pontua√ß√£o.
* Gera√ß√£o de estat√≠sticas por senten√ßa e por par√°grafo.
* Constru√ß√£o da distribui√ß√£o de comprimento das palavras.
Al√©m disso, o sistema incorpora an√°lise de desempenho de algoritmos de ordena√ß√£o, comparando *MergeSort* e *QuickSort* usando diferentes comparadores e vari√°veis de ordena√ß√£o. Para cada algoritmo, s√£o coletadas as seguintes m√©tricas:
* N√∫mero de compara√ß√µes
* N√∫mero de trocas
* Tempo de execu√ß√£o
Esses dados e a distribui√ß√£o por comprimento s√£o exportados no formato `.csv` e posteriormente utilizados para plotagem de gr√°ficos com scripts em Python.
Como parte da automa√ß√£o do projeto, foi desenvolvido pensando na execu√ß√£o em *pipeline* a partir do Makefile, tendo a seguinte ordem:
1. Compila√ß√£o e cria√ß√£o dos objetos (.o) do c√≥digo em C++.
2. Execu√ß√£o do analisador sobre o texto de entrada.
3. Gera√ß√£o do ``output.txt`` e dos arquivos `.csv`.
4. Plotagem dos gr√°ficos utilizando ``pandas`` e ``matplolib`` em Python.
Os textos utilizados para testar e analisar o projeto foram Dom Casmurro e a Semana Machado de Assis, ambas obras de Machado de Assis, oferecidas inicialmente para o trabalho de aproveitamento. Estes materiais foram selecionados devido ao tamanho textual e riqueza lingu√≠stica, possibilitando executar o sistema em um cen√°rio realista e desafiador para an√°lise.
Com este trabalho, busca-se consolidar os conceitos fundamentais da disciplina de LAEDI, demonstrando o conhecimento do autor sobre a disciplina em uma aplica√ß√£o real, mensur√°vel e extens√≠vel.

## üéØObjetivos

### Objetivo Geral
O objetivo geral deste trabalho √© projetar e implementar um Analisador L√©xico-Sint√°tico para processamento de texto em linguagem natural, capaz de identificar tokens, senten√ßas, par√°grafos, express√µes e padr√≥es sint√°ticos, utilizando estruturas de dados fundamentais e algoritmos de ordena√ß√£o, com foco em corretude, eleg√¢ncia e desempenho computacional.
Busca-se aplicar, de forma pr√°tica os conceitos de Algoritmos e Estruturas de Dados I, integrando an√°lise l√©xica, controle sint√°tico b√°sico (balanceamento de s√≠mbolos) e gera√ß√£o de estat√≠sticas textuais relevantes, podendo ser relacionado a disciplinas posteriores do curso, como Linguagens de Programa√ß√£o, Linguagens Formais e Aut√¥matos e Compiladores.
#### M√©tricas de Desempenho
A avalia√ß√£o do sistema desenvolvido considera os seguintes crit√©rios:
* **Tempo de Execu√ß√£o:**
  Tempo necess√°rio para realizar a an√°lise completa do texto e executar os algoritmos de ordena√ß√£o, medido em segundos.
* **Uso de Estruturas de Dados:**
  Aplica√ß√£o correta e elegante de estruturas de dados como pilhas, filas, listas encadeadas e tabelas hash, respeitando suas caracter√≠sticas.
* **Complexidade assint√≥tica:**
  An√°lise do comportamento assint√≥tico das principais opera√ß√µes, como inser√ß√µes em tabelas hash, percursos em listas encadeadas e algoritmos de ordena√ß√£o (*MergeSort* e *QuickSort*).
* **Qualidade da An√°lise Textual:**
  Capacidade do sistema em extrair informa√ß√µes l√©xicas e estruturais relevantes, como frequ√™ncia de tokens, distribui√ß√£o de comprimento das palavras e identifica√ß√£o de express√µes.

### Objetivos Espec√≠ficos
* **Implementar um analisador l√©xico para textos em linguagem natural:**
  Identificar e normalizar tokens (palavras), removendo *stopwords*, tratando abrevia√ß√µes, n√∫meros e caracteres especiais, al√©m de registrar ocorr√™ncias detalhadas (par√°grafo, senten√ßa, linha e posi√ß√£o).
* **Realizar an√°lise sint√°tica b√°sica do texto:**
  Verificar o balanceamento de s√≠mbolos de pontua√ß√£o (par√™nteses, colchetes, chaves, aspas), utilizando estruturas de pilha, identificando inconsist√™ncias sint√°ticas.
* **Organizar informa√ß√µes textuais por senten√ßas e par√°grafos:**
  Estruturar os dados analisados em n√≠veis hier√°rquicos (texto ‚Üí par√°grafos ‚Üí senten√ßas), armazenando estat√≠sticas como n√∫mero de palavras, palavras sem *stopwords* e comprimento m√©dio, al√©m de facilitar a gest√£o das estruturas de dados globais, como a listas de tabela hash para tokens e express√µes.
* **Detectar e contabilizar express√µes pr√©-definidas:**
  Identificar express√µes compostas ao longo do texto, registrando sua frequ√™ncia e linhas de ocorr√™ncia.
* **Construir distribui√ß√µes estat√≠sticas do texto:**
  Gerar a distribui√ß√£o de comprimento das palavras, utilizando uma estrutura de mapa (`IntIntMap`) implementada sem depend√™ncia da STL.
* **Aplicar e comparar algoritmos de ordena√ß√£o:**
  Implementar e avaliar *MergeSort* e *QuickSort* para diferentes crit√©rios de ordena√ß√£o (ordem alfab√©tica e frequ√™ncia), coletando m√©tricas de compara√ß√µes, trocas e tempo de execu√ß√£o.
* **Exportar dados para an√°lise externa:**
  Gerar arquivos `.csv` contendo m√©tricas de ordena√ß√£o e distribui√ß√µes estat√≠sticas, possibilitando a visualiza√ß√£o gr√°fica por meio de scripts em Python.
* **Documentar o projeto com Doxygen:**
  Produzir documenta√ß√£o t√©cnica completa do c√≥digo-fonte, incluindo descri√ß√£o de classes, m√©todos, par√¢metros e estruturas de dados utilizadas.

## üìö Fundamenta√ß√£o Te√≥rica
O desenvolvimento de um sistema de an√°lise textual eficiente exige a aplica√ß√£o integrada de conceitos cl√°ssicos de estruturas de dados, algoritmos de ordena√ß√£o e processamento de texto. Este trabalho fundamenta-se nesses pilares para realizar a leitura, organiza√ß√£o, an√°lise estat√≠stica e apresenta√ß√£o de informa√ß√µes extra√≠das de textos extensos, respeitando crit√©rios de desempenho e uso eficiente de mem√≥ria.

A seguir, s√£o apresentados os principais conceitos te√≥ricos que embasam a implementa√ß√£o do sistema proposto.

### An√°lise l√©xica (Scanner)
√â o processo de decompor um texto ou c√≥digo-fonte em unidades m√≠nimas e significativas, denominadas *tokens* que s√£o usadas fundamentalmente em compiladores, interpretadores e em processamento de linguagem natural (PLN), para entender a estrutura e significado da entrada.

### An√°lise sint√°tica (Parser)
Tem como fun√ß√£o receber os tokens do analisador l√©xico e verificar se eles formam uma estrutura gramaticalmente correta, de acordo com as regras da linguagem. Tamb√©m s√£o fundamentais para compiladores e PLN.

### Pilhas (Stacks)
A pilha √© uma estrutura de dados baseada no princ√≠pio FILO (First In, Last Out). Os elementos s√£o inseridos e removidos sempre pelo topo.
Neste projeto, pilhas s√£o usadas para:
* Verificar o balanceamento de s√≠mbolos de pontua√ß√£o, como par√™nteses, colchetes e chaves;
* Garantir a correta correspond√™ncia entre s√≠mbolos de abertura e fechamento.

### Filas (Queues)
A fila segue o princ√≠pio FIFO (First In, First Out), em que o primeiro elemento inserido (enqueue) √© o primeiro a ser removido (dequeue).
As filas s√£o empregadas para:
* Armazenar senten√ßas e par√°grafos na ordem em que aparecem no texto;
* Garantir o processamento sequencial correto das estruturas textuais.

### Listas Encadeadas (Linked Lists)
As listas encadeadas permitem inser√ß√£o e remo√ß√£o din√¢mica de elementos, sem necessidade de realoca√ß√£o cont√≠nua de mem√≥ria. Cada elemento aponta para o pr√≥ximo, formando uma sequ√™ncia encadeada.
No contexto deste projeto, as listas encadeadas s√£o utilizadas para:
* Armazenar tokens e express√µes;
* Registrar ocorr√™ncias (linhas, posi√ß√µes, senten√ßas);
* Servir como base para estruturas mais complexas.

### Tabelas Hash (Hash Tables)
Tabelas hash s√£o estruturas de dados que permitem acesso r√°pido a elementos por meio de uma fun√ß√£o de dispers√£o (hash function). Elas oferecem, em m√©dia, complexidade de tempo constante para inser√ß√£o, busca e remo√ß√£o. A resolu√ß√£o de colis√µes √© feita por encadeamento, utilizando listas encadeadas nos buckets da tabela.
No projeto, tabelas hash s√£o utilizadas para:
* Armazenar tokens (palavras) e suas estat√≠sticas;
* Registrar express√µes e suas ocorr√™ncias;
* Evitar duplica√ß√£o de entradas;
* Facilitar consultas r√°pidas por texto.

### Mapas Associativos
Mapas associativos permitem relacionar chaves a valores. Neste trabalho, foi implementado um mapa espec√≠fico de inteiros (`IntIntMap`) sem o uso da biblioteca padr√£o STL.
Esse mapa √© utilizado para:
* Construir a distribui√ß√£o de comprimento das palavras;
* Associar o tamanho da palavra √† sua frequ√™ncia de ocorr√™ncia.

### Algoritmos de Ordena√ß√£o
A ordena√ß√£o dos dados √© essencial para a apresenta√ß√£o organizada dos resultados e para an√°lises comparativas de desempenho.
#### Merge Sort
O Merge Sort √© um algoritmo de ordena√ß√£o baseado na estrat√©gia "dividir para conquistar". Ele divide o vetor em partes menores, ordena cada parte e, em seguida, realiza a fus√£o ordenada.
Caracter√≠sticas:
* Complexidade do pior e melhor caso: $$O(n\;log\;n)$$
* Est√°vel
* Custo adicional de mem√≥ria para copiar o vetor.
* N√£o h√° melhora se os elementos j√° estiverem parcialmente ordenados.
* N√£o faz troca de elementos, pois eles se ordenam quando as partes se fundem (merge).
#### Quick Sort
O Quick Sort tamb√©m segue a estrat√©gia de "dividir para conquistar", escolhendo um piv√¥ e particionando o vetor em elementos menores e maiores que ele.
Caracter√≠sticas:
* Complexidade m√©dia: $$O(n\;log\;n)$$
* Pior caso: $$O(n^2)$$
* Inst√°vel, pode alterar a ordem dos elementos de mesmo valor.
* Custo de mem√≥ria extra relacionado a recurs√£o na pilha.
#### Merge Sort X Quick Sort
|Caracter√≠stica|**MergeSort**| **QuickSort**|
|-|-|-|
|**Estrat√©gia**|Dividir  para conquistar|Dividir  para conquistar|
|**Complexidade (Melhor Caso)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|
|**Complexidade (Caso M√©dio)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|
|**Complexidade (Pior Caso)**|$$O(n\;log\;n)$$|$$O(n^2)$$|
|**Estabilidade**|Est√°vel|Inst√°vel|
|**Adapatabilidade**|N√£o|Sim|
|**Mem√≥ria extra**|$$O(n)$$|$$O(log\;n)$$|
|**Movimenta√ß√µes (Swaps)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|

**Tabela comparativa das caracteriscas do Merge Sort e do Quick Sort**

## üî¨ Modelagem da Aplica√ß√£o
A partir do objetivo de desenvolver um sistema analisador l√©xico-sint√°tico, a modelagem teve como princ√≠pio equilibrar o desempenho computacional com a constru√ß√£o de um sistema robusto e modular. O projeto foi implementado em C++, visando efici√™ncia computacional, controle expl√≠cito de mem√≥ria e flexibilidade na implementa√ß√£o de estruturas personalizadas com POO.

A arquitetura do sistema segue uma abordagem modular, na qual cada componente possui responsabilidades bem definidas, facilitando tanto a manuten√ß√£o quanto a an√°lise individual de desempenho. O fluxo geral da aplica√ß√£o √© coordenado pela fun√ß√£o `main`, que orquestra a leitura do texto, a an√°lise l√©xica, o armazenamento dos dados e a gera√ß√£o dos relat√≥rios e m√©tricas experimentais.

### Fluxo de Execu√ß√£o do Sistema
O funcionamento do analisador pode ser dividido em etapas sequenciais, que refletem o *pipeline* de processamento textual adotado no projeto:
1. **Leitura do Texto de Entrada**
   O sistema inicia com a leitura do arquivo de texto fornecido via linha de comando. Essa etapa √© realizada pela classe `TextReader`, respons√°vel por:

   * Abrir e validar o arquivo de entrada;
   * Ler o texto linha a linha;
   * Manter o controle do n√∫mero da linha atual.
2. **An√°lise L√©xica**
   A etapa central do sistema √© conduzida pela classe `Analyzer`, que processa cada linha do texto e executa as seguintes opera√ß√µes:

   * Normaliza√ß√£o das palavras (remo√ß√£o de acentos, convers√£o para min√∫sculas, tratamento de caracteres especiais);
   * Identifica√ß√£o e filtragem de *stopwords*, definidas em `stopwords.txt`;
   * Tokeniza√ß√£o do texto em palavras (*tokens*);
   * Registro das ocorr√™ncias de cada token (par√°grafo, senten√ßa, linha e posi√ß√£o);
   * Detec√ß√£o de senten√ßas e par√°grafos;
   * Verifica√ß√£o do balanceamento de s√≠mbolos de pontua√ß√£o por meio de pilhas;
   * Identifica√ß√£o e contagem de express√µes previamente definidas em `expressoes.txt`.
3. **Armazenamento em Estruturas de Dados**
   Ainda na class ``Analyzer``, os dados extra√≠dos durante a an√°lise s√£o armazenados utilizando estruturas implementadas manualmente no projeto, tais como:

   * Tabelas hash para tokens e express√µes;
   * Listas encadeadas para armazenar cole√ß√µes din√¢micas;
   * Filas para preservar a ordem de senten√ßas e par√°grafos;
   * Pilhas para an√°lise de balanceamento de s√≠mbolos.
4. **Gera√ß√£o de Relat√≥rios e Exporta√ß√£o de Dados**
   A classe `Report` √© respons√°vel por:

   * Gerar relat√≥rios textuais detalhados com os resultados da an√°lise;
   * Exportar m√©tricas de desempenho e distribui√ß√µes estat√≠sticas em arquivos `.csv`;
   * Produzir dados que posteriormente s√£o utilizados para a gera√ß√£o de gr√°ficos.
5. **Visualiza√ß√£o Gr√°fica (P√≥s-processamento)**
   Scripts auxiliares em Python utilizam os arquivos `.csv` gerados para:

   * Plotar gr√°ficos de distribui√ß√£o do comprimento das palavras;
   * Comparar o desempenho dos algoritmos de ordena√ß√£o em fun√ß√£o do tamanho da entrada.

Essa separa√ß√£o permite o desenvolvimento e teste modular da aplica√ß√£o, al√©m de facilitar a manuten√ß√£o e refatora√ß√£o do c√≥digo, caso necess√°rio.

**Fluxograma de execu√ß√£o do projeto.**

### üìä Estruturas de Dados

O projeto faz uso extensivo de estruturas de dados implementadas manualmente, evitando o uso direto de containers prontos da STL para fins educacionais. Entre as principais estruturas utilizadas, destacam-se:

* **Lista Encadeada (`LinkedList`)**
  Utilizada como base para diversas outras estruturas, permitindo armazenamento din√¢mico de elementos. Utiliza o recurso de templates do C++, tornando-a uma estrutura gen√©rica, podendo listar qualquer tipo de elemento. Possui complexidade de inser√ß√£o, remo√ß√£o e acesso de O(n).

* **Tabela Hash (`HashTable`)**
  Empregada para armazenar tokens e express√µes, permitindo acesso eficiente por chave textual. Tamb√©m utiliza de template, por√©m √© generalizada somente para elementos que possuem o m√©todo `addOccurrence`. Possui complexidade de inser√ß√£o e acesso O(1) no melhor caso e O(n) no pior caso, sendo n o tamanho do *bucket*.

* **Fila (`Queue`)**
  Utilizada para manter a ordem de senten√ßas e par√°grafos durante o processamento e a gera√ß√£o de relat√≥rios. Possui complexidade de inser√ß√£o, acesso e remo√ß√£o O(1), gra√ßas aos ponteiros de `front` e `rear`.

* **Pilha (`Stack`)**
  Aplicada na verifica√ß√£o do balanceamento de s√≠mbolos de pontua√ß√£o, como par√™nteses e colchetes. Possui complexidade de inser√ß√£o, acesso e remo√ß√£o O(1).

* **Mapeamento Inteiro‚ÄìInteiro (`IntIntMap`)**
  Utilizado para armazenar distribui√ß√µes estat√≠sticas, como a frequ√™ncia de palavras por comprimento. Possui inser√ß√£o e acesso O(1).

Essas estruturas foram projetadas visando clareza conceitual, efici√™ncia e integra√ß√£o com os algoritmos de ordena√ß√£o e an√°lise estat√≠stica.

### üöÄ Otimiza√ß√µes Propostas

#### Pr√©-processamento Textual

* Normaliza√ß√£o antecipada das palavras para reduzir compara√ß√µes redundantes;
* Remo√ß√£o de *stopwords* ainda na fase de an√°lise, reduzindo o volume de dados armazenados.

#### Uso de Hashing

* Utiliza√ß√£o de fun√ß√£o hash simples e eficiente para distribui√ß√£o uniforme dos tokens;
* Redu√ß√£o do custo m√©dio de inser√ß√£o e busca na tabela de s√≠mbolos.

#### An√°lise Emp√≠rica de Ordena√ß√£o

* Execu√ß√£o de m√∫ltiplos testes com diferentes tamanhos de entrada;
* Compara√ß√£o entre MergeSort e QuickSort sob diferentes crit√©rios de ordena√ß√£o;
* Coleta detalhada de m√©tricas para valida√ß√£o emp√≠rica das complexidades assint√≥ticas esperadas.

## üìù Metodologia
A implementa√ß√£o do sistema de an√°lise l√©xica e sint√°tica foram feitas em C++, uitilizando a IDE Visual Studio Code para desenvolvimento do c√≥digo-fonte e uso do Github para controle de vers√£o. O projeto foi organizado em um reposit√≥rio, contendo diret√≥rios dividindo os arquivos de cabe√ßalho (.hpp) em ``include``, arquivos de implementa√ß√£o (.cpp) em ``src``, arquivos de entrada em ``data``, scripts em Python no diret√≥rio ``utils``, diret√≥rio de sa√≠da ``output``, al√©m de outros diret√≥rios auxiliares.

## üìÅ Estrutura do Projeto
A seguir, a estrutura do diret√≥rio do projeto, organizada para separar o c√≥digo-fonte, os dados e os resultados:
```
.
‚îÇ
‚îú‚îÄ‚îÄ bin/
|   ‚îî‚îÄ‚îÄ LSA
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ DomCasmurro.txt
‚îÇ   ‚îú‚îÄ‚îÄ expressoes.txt
‚îÇ   ‚îú‚îÄ‚îÄ output.txt
|   ‚îú‚îÄ‚îÄ Semana_Machado_Assis.txt
‚îÇ   ‚îî‚îÄ‚îÄ stopwords.txt
‚îÇ
‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îú‚îÄ‚îÄ Analyzer.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Expression.hpp
‚îÇ   ‚îú‚îÄ‚îÄ ExpressionComparators.hpp
‚îÇ   ‚îú‚îÄ‚îÄ HashTable.hpp
‚îÇ   ‚îú‚îÄ‚îÄ IntIntMap.hpp
‚îÇ   ‚îú‚îÄ‚îÄ LinkedList.hpp
‚îÇ   ‚îú‚îÄ‚îÄ MapEntry.hpp
‚îÇ   ‚îú‚îÄ‚îÄ MapEntryComparators.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Node.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Occurrence.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Paragraph.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Queue.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Report.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Sentence.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Sorter.hpp
‚îÇ   ‚îú‚îÄ‚îÄ SortMetrics.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Stack.hpp
‚îÇ   ‚îú‚îÄ‚îÄ TextReader.hpp
‚îÇ   ‚îú‚îÄ‚îÄ Token.hpp
|   ‚îî‚îÄ‚îÄ TokenComparators.hpp
‚îÇ
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îú‚îÄ‚îÄ length_dist.csv
‚îÇ   ‚îú‚îÄ‚îÄ length_distribution.png
‚îÇ   ‚îú‚îÄ‚îÄ output.txt
‚îÇ   ‚îú‚îÄ‚îÄ sort_metrics.csv
‚îÇ   ‚îú‚îÄ‚îÄ sort_performance_comparisons.png
‚îÇ   ‚îú‚îÄ‚îÄ sort_performance_swaps.png
‚îÇ   ‚îî‚îÄ‚îÄ sort_performance_time.png
|
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ Analyzer.cpp
‚îÇ   ‚îú‚îÄ‚îÄ Expression.cpp
‚îÇ   ‚îú‚îÄ‚îÄ IntIntMap.cpp
‚îÇ   ‚îú‚îÄ‚îÄ main.cpp
‚îÇ   ‚îú‚îÄ‚îÄ Paragraph.cpp
‚îÇ   ‚îú‚îÄ‚îÄ Report.cpp
‚îÇ   ‚îú‚îÄ‚îÄ Sentence.cpp
‚îÇ   ‚îú‚îÄ‚îÄ TextReader.cpp
‚îÇ   ‚îî‚îÄ‚îÄ Token.cpp
‚îÇ
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ plot_length.py
‚îÇ   ‚îú‚îÄ‚îÄ plot_sort_metrics.py
‚îÇ   ‚îî‚îÄ‚îÄ plot_utils.py
|
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Doxyfile
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ Makefile
‚îî‚îÄ‚îÄ README.md
```
### üìÅ Arquivos e Diret√≥rios
A seguir, s√£o descritos os principais arquivos e diret√≥rios do projeto, detalhando suas responsabilidades no processo de an√°lise l√©xica e sint√°tica.
* **`bin/`**
  Cont√©m o execut√°vel final do projeto ap√≥s a compila√ß√£o.
  * `LSA`: execut√°vel do *Lexical-Syntactic Analyzer*.
* **`data/`**
  Diret√≥rio que armazena os arquivos de entrada utilizados na an√°lise textual:
  * `DomCasmurro.txt`: texto liter√°rio utilizado como base principal para an√°lise.
  * `expressoes.txt`: arquivo contendo express√µes espec√≠ficas a serem identificadas no texto.
  * `output.txt`: arquivo de exemplo utilizado para montar o ``output.txt`` do projeto.
  * `Semana_Machado_Assis.txt`: texto alternativo para testes comparativos.
  * `stopwords.txt`: lista de palavras irrelevantes (artigos, preposi√ß√µes etc.) removidas durante a an√°lise l√©xica.
* **`include/`**
  Cont√©m todos os arquivos de cabe√ßalho (`.hpp`), respons√°veis pela defini√ß√£o das estruturas de dados, classes, classes template e interfaces do sistema.
* **`src/`**
  Cont√©m os arquivos de implementa√ß√£o (`.cpp`) correspondentes aos cabe√ßalhos definidos em `include/`.
* **`output/`**
  Diret√≥rio de sa√≠da que armazena os resultados da execu√ß√£o do analisador, incluindo arquivos de texto, m√©tricas e gr√°ficos gerados:
  * `output.txt`: relat√≥rio textual com os resultados da an√°lise l√©xica e sint√°tica.
  * `length_dist.csv`: distribui√ß√£o de tokens por comprimento.
  * `sort_metrics.csv`: m√©tricas coletadas durante os algoritmos de ordena√ß√£o.
  * Arquivos `.png`: gr√°ficos de desempenho e an√°lise estat√≠stica.
* **`utils/`**
  Scripts auxiliares em Python utilizados para plotagem de gr√°ficos e an√°lise visual dos dados:
  * `plot_length.py`: gera gr√°fico da distribui√ß√£o de comprimentos dos tokens.
  * `plot_sort_metrics.py`: gera gr√°ficos comparativos de desempenho dos algoritmos de ordena√ß√£o.
  * `plot_utils.py`: fun√ß√µes auxiliares comuns aos scripts de plotagem.

### üìö Bibliotecas
A implementa√ß√£o do Analisador L√©xico e Sint√°tico n√£o utiliza das estruturas de dados do STL, por√©m utiliza de outras bibliotecas padr√µes do C++. As principais bibliotecas s√£o:

### üìå Bibliotecas do Projeto (Headers Customizados)
Os principais arquivos de cabe√ßalho desenvolvidos no projeto s√£o:
* **`Analyzer.hpp`**
  Define a classe central do sistema, respons√°vel por orquestrar a an√°lise l√©xica e sint√°tica, integrando a leitura do texto, tokeniza√ß√£o, filtragem de *stopwords*, identifica√ß√£o de express√µes e gera√ß√£o dos relat√≥rios finais.
* **`TextReader.hpp`**
  Respons√°vel pela leitura dos arquivos de entrada e controle da linha atual e o n√∫mero da linha.
* **`Token.hpp`**
  Define a estrutura de um token l√©xico, contendo informa√ß√µes como o texto, e a lista de ocorrencias do token.
* **`Sentence.hpp`** e **`Paragraph.hpp`**
  Representam a organiza√ß√£o hier√°rquica do texto, permitindo armazenar informa√ß√µes de senten√ßas e par√°grafos para uso posterior em fila.

* **`Expression.hpp`**
  Representa express√µes compostas detectadas no texto, com uma lista de linhas onde foram encontradas determinada express√£o..

* **`Occurrence.hpp`**
  Armazena informa√ß√µes sobre as ocorr√™ncias de tokens, como senten√ßa, par√°grafo, linha e posi√ß√£o.

* **`HashTable.hpp`**
  Implementa√ß√µes customizadas de tabelas hash gen√©rico apenas para objetos com m√©todo `addOccurrence`.

**`IntIntMap.hpp`**
  Implementa√ß√£o de mapa de inteiro para inteiro, utilizado para calcular distribui√ß√£o por comprimento dos tokens do texto de entrada.

**`Node.hpp`**
  Implementa√ß√£o de n√≥ simples gen√©rico, contendo o dado template `T` e o ponteiro para pr√≥ximo n√≥.

* **`LinkedList.hpp`**
  Implementa listas encadeadas gen√©ricas utilizadas para armazenar cole√ß√µes din√¢micas de tokens, express√µes e ocorr√™ncias.

* **`Stack.hpp`** e **`Queue.hpp`**
  Estruturas auxiliares empregadas em etapas espec√≠ficas da an√°lise sint√°tica e no controle de fluxo de dados.

* **`Sorter.hpp`**
  Implementa os dois algoritmos de ordena√ß√£o selecionados, aplicados aos tokens, express√µes e entradas do mapa, permitindo a an√°lise de desempenho conforme crit√©rios alfab√©ticos e por frequ√™ncia.

* **`SortMetrics.hpp`**
  Estrutura respons√°vel por coletar m√©tricas de desempenho dos algoritmos de ordena√ß√£o, como n√∫mero de compara√ß√µes, trocas e tempo de execu√ß√£o.

* **`Report.hpp`**
  Respons√°vel pela consolida√ß√£o dos resultados da an√°lise e pela gera√ß√£o dos arquivos de sa√≠da em formato textual.

* **Arquivos de Comparadores (`ExpressionComparators.hpp`, `TokenComparators.hpp`, `MapEntryComparators.hpp`)**
  Definem crit√©rios personalizados de compara√ß√£o utilizados nos processos de ordena√ß√£o e ranqueamento.

### üìå Bibliotecas Padr√£o do C++

A seguir est√£o listadas as principais bibliotecas padr√£o do C++ utilizadas no projeto:

* **`iostream`**
  Utilizada para entrada e sa√≠da de dados no console, especialmente para mensagens de execu√ß√£o e depura√ß√£o.

* **`fstream`**
  Empregada na leitura dos arquivos de entrada (`.txt`) e na escrita dos arquivos de sa√≠da gerados pelo analisador.

* **`string`**
  Essencial para a manipula√ß√£o de cadeias de caracteres durante a tokeniza√ß√£o e an√°lise l√©xica.

* **`chrono`**
  Utilizado para medi√ß√£o precisa do tempo de execu√ß√£o dos algoritmos de ordena√ß√£o, permitindo a an√°lise de desempenho.

* **`cctype`**
  Utilizado para classifica√ß√£o de caracteres durante a an√°lise l√©xica (letras, d√≠gitos, pontua√ß√£o, etc.).

* **`iomanip`**
  Utilizado para formata√ß√£o da sa√≠da de dados num√©ricos nos relat√≥rios.

* **`cstdlib`** e **`cstring`**
  Utilizados para opera√ß√µes b√°sicas de convers√£o e manipula√ß√£o de dados em baixo n√≠vel quando necess√°rio.

## ‚öôÔ∏è Estruturas de Dados e Implementa√ß√£o do Sistema
Nesta se√ß√£o s√£o descritas e explicadas as classes e estruturas de dados desenvolvidas para o projeto, evidenciando seus atributos e m√©todos.

---
### üß± Estruturas Base do Sistema

---
#### `struct Occurrence`
Representa uma ocorr√™ncia individual de um token no texto.

**Atributos:**
* `paragraph`: N√∫mero do par√°grafo onde ocorre.
* `sentence`: N√∫mero da senten√ßa onde ocorre.
* `line`: N√∫mero da linha onde ocorre.
* `position`: Posi√ß√£o do token dentro da senten√ßa.

**M√©todos:**
* `Occurrence(int p, int s, int l, int pos)`: Construtor que inicializa todos os campos posicionais da ocorr√™ncia.

---
#### `class Token`
Representa um token l√©xico (palavra) extra√≠do do texto.

**Atributos:**
* `text`: Texto do token.
* `frequency`: N√∫mero total de ocorr√™ncias do token no texto.
* `occurrences`: Lista encadeada contendo todas as ocorr√™ncias do token.

**M√©todos:**
* `Token()`: Construtor padr√£o.
* `Token(const string &t)`: Construtor que inicializa o token com um texto espec√≠fico.
* `Token(const Token &other)`: Construtor de c√≥pia.
* `Token& operator=(const Token &other)`: Operador de atribui√ß√£o para c√≥pia de tokens.
* `addOccurrence(int p, int s, int l, int pos)`: Registra uma nova ocorr√™ncia do token, atualizando a frequ√™ncia.
* `getText()`: Retorna o texto do token.
* `getFrequency()`: Retorna a frequ√™ncia total do token.
* `getOccurrences()`: Retorna a lista de ocorr√™ncias associadas ao token.

---
#### `class Sentence`
Armazena informa√ß√µes estat√≠sticas associadas a uma senten√ßa do texto.

**Atributos:**
* `paragraphNumber`: N√∫mero do par√°grafo ao qual a senten√ßa pertence.
* `sentenceNumber`: √çndice da senten√ßa dentro do par√°grafo.
* `stopWords`: Quantidade de *stopwords* na senten√ßa.
* `nonStopWords`: Quantidade de palavras relevantes na senten√ßa.
* `averageWordLength`: Comprimento m√©dio das palavras da senten√ßa.

**M√©todos:**
* `Sentence()`: Construtor padr√£o.
* `Sentence(int pN, int sN, int sWords, int nWords, double avrWordLength)`: Construtor que inicializa todas as m√©tricas da senten√ßa.
* `getParagraphNumber()`: Retorna o n√∫mero do par√°grafo.
* `getSentenceNumber()`: Retorna o n√∫mero da senten√ßa.
* `getAllWords()`: Retorna o total de palavras da senten√ßa (com e sem *stopwords*).
* `getNormalWords()`: Retorna o n√∫mero de palavras n√£o consideradas *stopwords*.
* `getAverageWordLength()`: Retorna o comprimento m√©dio das palavras.

---
#### `class Paragraph`
Representa um par√°grafo do texto analisado.

**Atributos:**
* `number`: N√∫mero sequencial do par√°grafo no texto.
* `startingLine`: N√∫mero da linha onde o par√°grafo se inicia.
* `totalSentences`: Quantidade de senten√ßas no par√°grafo.

**M√©todos:**
* `Paragraph()`: Construtor padr√£o.
* `Paragraph(int n, int sL, int tS)`: Construtor que inicializa os metadados do par√°grafo.
* `getNumber()`: Retorna o n√∫mero do par√°grafo.
* `getStartingLine()`: Retorna a linha inicial do par√°grafo.
* `getTotalSentences()`: Retorna o n√∫mero total de senten√ßas.

---
#### `class Expression`
Representa uma express√£o composta por m√∫ltiplas palavras.

**Atributos:**
* `text`: Texto completo da express√£o.
* `frequency`: N√∫mero de ocorr√™ncias da express√£o no texto.
* `lines`: Lista encadeada contendo as linhas onde a express√£o ocorre.

**M√©todos:**
* `Expression()`: Construtor padr√£o.
* `Expression(const string &t)`: Construtor que inicializa a express√£o com um texto espec√≠fico.
* `Expression(const Expression &other)`: Construtor de c√≥pia.
* `Expression& operator=(const Expression &other)`: Operador de atribui√ß√£o.
* `addOccurrence(int line)`: Registra uma nova ocorr√™ncia da express√£o.
* `getText()`: Retorna o texto da express√£o.
* `getFrequency()`: Retorna a frequ√™ncia da express√£o.
* `getLines()`: Retorna a lista de linhas onde a express√£o ocorre.

---
### üß± Estruturas de Dados Gen√©ricas

---
#### `struct Node<T>`
Estrutura fundamental para a implementa√ß√£o de listas encadeadas simples, servindo como base para estruturas lineares como pilhas, filas e listas.
A implementa√ß√£o expl√≠cita da estrutura `Node` permite compreender e controlar diretamente o gerenciamento din√¢mico de mem√≥ria, al√©m de servir como elemento unificador para diferentes estruturas de dados lineares, evitando duplica√ß√£o de c√≥digo e refor√ßando o conceito de abstra√ß√£o estrutural.

**Atributos:**
* `data`: Elemento armazenado no n√≥.
* `next`: Ponteiro para o pr√≥ximo n√≥ da estrutura.

**M√©todos:**
* `Node(const T &value)`: Construtor que inicializa o n√≥ com um valor e ponteiro nulo para o pr√≥ximo elemento.

---
#### `class Stack<T>`
Implementa√ß√£o gen√©rica de uma pilha seguindo a pol√≠tica FILO.

**Atributos:**
* `topNode`: Ponteiro para o elemento do topo da pilha.
* `size`: N√∫mero total de elementos armazenados.

**M√©todos:**
* `Stack()`: Construtor padr√£o.
* `Stack(const Stack<T> &other)`: Construtor de c√≥pia preservando a ordem dos elementos.
* `~Stack()`: Destrutor que libera toda a mem√≥ria alocada.
* `operator=`: Operador de atribui√ß√£o.
* `push(const T &value)`: Insere um elemento no topo da pilha (*O(1)*).
* `pop(T &removed)`: Remove o elemento do topo (*O(1)*).
* `peek(T &value)`: Consulta o elemento do topo sem remov√™-lo (*O(1)*).
* `isEmpty()`: Verifica se a pilha est√° vazia.
* `getSize()`: Retorna o n√∫mero de elementos.
* `clear()`: Remove todos os elementos da pilha.

---
#### `class Queue<T>`
Implementa√ß√£o gen√©rica de uma fila seguindo a pol√≠tica FIFO.

**Atributos:**
* `front`: Ponteiro para o primeiro elemento da fila.
* `rear`: Ponteiro para o √∫ltimo elemento da fila.
* `size`: N√∫mero total de elementos na fila.

**M√©todos:**
* `Queue()`: Construtor padr√£o.
* `Queue(const Queue<T> &other)`: Construtor de c√≥pia.
* `~Queue()`: Destrutor que libera a mem√≥ria.
* `operator=`: Operador de atribui√ß√£o.
* `enqueue(const T &value)`: Insere um elemento no final da fila (*O(1)*).
* `dequeue(T &removed)`: Remove o elemento do in√≠cio da fila (*O(1)*).
* `isEmpty()`: Verifica se a fila est√° vazia.
* `getSize()`: Retorna o n√∫mero de elementos.
* `clear()`: Remove todos os elementos da fila.

---
#### `class LinkedList<T>`
Implementa√ß√£o gen√©rica de uma lista encadeada simples.

**Atributos:**
* `head`: Ponteiro para o primeiro n√≥ da lista.
* `size`: N√∫mero de elementos armazenados.

**M√©todos:**
* `LinkedList()`: Construtor padr√£o.
* `LinkedList(const LinkedList<T> &other)`: Construtor de c√≥pia.
* `~LinkedList()`: Destrutor.
* `operator=`: Operador de atribui√ß√£o.
* `insert(const T &value)`: Insere um elemento no final da lista (*O(n)*).
* `get(int i)`: Retorna o elemento na posi√ß√£o `i`.
* `isEmpty()`: Verifica se a lista est√° vazia.
* `getSize()`: Retorna o tamanho da lista.
* `clear()`: Remove todos os elementos da lista.
* `toArray(int &outSize)`: Converte a lista em um vetor din√¢mico.
* `Iterator`: Classe interna que permite itera√ß√£o sequencial sobre os elementos da lista.

---
#### `class HashTable<T>`
Implementa√ß√£o de tabela hash com "encadeamento separado" para tratamento de colis√µes.
Nesta implementa√ß√£o, a estrutura n√£o √© totalmente gen√©rica, pois assume que o tipo armazenado possui m√©todos como `getText()` e construtores espec√≠ficos, refletindo uma adapta√ß√£o pr√°tica ao dom√≠nio do problema.

**Atributos:**
* `TABLE_SIZE`: Tamanho fixo da tabela.
* `table`: Vetor de listas encadeadas usado como buckets.

**M√©todos:**
* `HashTable()`: Construtor padr√£o.
* `HashTable(const HashTable &other)`: Construtor de c√≥pia.
* `~HashTable()`: Destrutor.
* `insert(const string &key, int ...)`: Insere tokens com suas ocorr√™ncias.
* `insert(const string &key, int line)`: Insere express√µes e suas ocorr√™ncias.
* `getBucket(int index)`: Retorna um bucket espec√≠fico.
* `getTableSize()`: Retorna o tamanho da tabela.
* `countObjects()`: Conta o total de elementos armazenados.
* `contains(const string &key)`: Verifica se uma chave existe.
* `toArray(int &outSize)`: Converte a tabela hash em um vetor din√¢mico.
* `clear()`: Remove todos os elementos da tabela.

---
### üó∫Ô∏è Estruturas Auxiliares de Mapeamento

---
#### `struct MapEntry`
Estrutura que representa um par chave‚Äìvalor inteiro, utilizada como elemento b√°sico em mapas personalizados.
A separa√ß√£o expl√≠cita da estrutura `MapEntry` permite desacoplar o conceito de *entrada de mapa* da implementa√ß√£o da estrutura de dados que a armazena. Isso facilita a reutiliza√ß√£o em diferentes contextos, al√©m de tornar o c√≥digo mais modular e semanticamente claro em aplica√ß√µes de contagem e distribui√ß√£o de frequ√™ncias.

**Atributos:**
* `key`: Chave inteira associada √† entrada do mapa.
* `value`: Valor inteiro associado √† chave, normalmente utilizado como contador de frequ√™ncia.

**M√©todos:**
* `MapEntry(int key = 0, int value = 0)`: Construtor que inicializa a chave e o valor da entrada.

---
#### `class IntIntMap`

Implementa√ß√£o personalizada de um mapa (inteiro ‚Üí inteiro), baseada em lista encadeada.
A cria√ß√£o do `IntIntMap` se justifica pela necessidade de evitar o uso da STL e, ao mesmo tempo, oferecer uma estrutura simples e controlada para contagem de frequ√™ncias. Embora apresente complexidade linear, essa abordagem √© adequada para conjuntos de dados moderados e refor√ßa o entendimento dos mecanismos internos de estruturas associativas.
Al√©m disso, essa implementa√ß√£o resolve limita√ß√µes da `HashTable<T>` gen√©rica, que n√£o suporta naturalmente pares chave‚Äìvalor, sendo necess√°ria uma estrutura especializada para mapear inteiros de forma direta e sem depend√™ncias externas.

**Atributos:**
* `list`: Lista encadeada de `MapEntry`, respons√°vel por armazenar os pares chave‚Äìvalor.

**M√©todos:**
* `IntIntMap()`: Construtor padr√£o.
* `~IntIntMap()`: Destrutor.
* `increment(int key)`: Incrementa o valor associado √† chave em uma unidade (*O(n)*).
* `increment(int key, int amount)`: Incrementa o valor associado √† chave por um valor arbitr√°rio (*O(n)*).
* `get(int key)`: Retorna o valor associado √† chave, ou `0` caso n√£o exista (*O(n)*).
* `size()`: Retorna o n√∫mero de entradas armazenadas (*O(1)*).
* `getEntries()`: Retorna uma refer√™ncia para a lista de entradas.
* `clear()`: Remove todas as entradas do mapa.

---
### üîÄ Estruturas de Compara√ß√£o e Ordena√ß√£o
A separa√ß√£o das fun√ß√µes de compara√ß√£o em um m√≥dulo espec√≠fico promove desacoplamento entre os algoritmos de ordena√ß√£o e os crit√©rios de ordena√ß√£o. Isso permite reutilizar os mesmos algoritmos (Merge Sort e Quick Sort) com diferentes pol√≠ticas de ordena√ß√£o, al√©m de facilitar a coleta de m√©tricas experimentais.

---
#### Fun√ß√µes de Compara√ß√£o para `Expression` (`ExpressionComparators.hpp`)
##### `expressionAlpha`
* **Fun√ß√£o:** Comparador alfab√©tico para objetos `Expression`.
* **Crit√©rio:** Ordem lexicogr√°fica crescente do texto da express√£o.
* **Aspecto anal√≠tico:** Incrementa o contador de compara√ß√µes em `SortMetrics`.

##### `expressionFreq`
* **Fun√ß√£o:** Comparador por frequ√™ncia de ocorr√™ncia.
* **Crit√©rio:** Ordem decrescente de frequ√™ncia.
* **Aspecto anal√≠tico:** Incrementa o contador de compara√ß√µes para an√°lise de desempenho.

---
#### Fun√ß√µes de Compara√ß√£o para `Token` (`TokenComparators.hpp`)
##### `tokenAlpha`
* **Fun√ß√£o:** Comparador alfab√©tico entre tokens.
* **Crit√©rio:** Ordem lexicogr√°fica crescente do texto do token.
* **Aspecto anal√≠tico:** Atualiza o n√∫mero de compara√ß√µes em `SortMetrics`.

##### `tokenFreq`
* **Fun√ß√£o:** Comparador por frequ√™ncia de ocorr√™ncia.
* **Crit√©rio:** Ordem decrescente da frequ√™ncia do token.
* **Aspecto anal√≠tico:** Utilizado para an√°lises de palavras mais recorrentes.

---
#### Fun√ß√µes de Compara√ß√£o para `MapEntry` (`MapEntryComparators.hpp`)
##### `mapEntryKeyAsc`
* **Fun√ß√£o:** Comparador por chave.
* **Crit√©rio:** Ordem crescente da chave inteira.
* **Uso t√≠pico:** An√°lise ordenada de categorias ou intervalos.

##### `mapEntryValueDesc`
* **Fun√ß√£o:** Comparador por valor.
* **Crit√©rio:** Ordem decrescente do valor.
* **Uso t√≠pico:** Identifica√ß√£o de frequ√™ncias dominantes.

---
#### `struct SortMetrics`
Estrutura respons√°vel por armazenar m√©tricas de desempenho de algoritmos de ordena√ß√£o.
A instrumenta√ß√£o expl√≠cita dos algoritmos permite a avalia√ß√£o emp√≠rica de desempenho, essencial em disciplinas de An√°lise de Algoritmos e Estruturas de Dados. Essa abordagem possibilita comparar algoritmos sob diferentes entradas e crit√©rios de ordena√ß√£o.

**Atributos:**
* `comparisons`: N√∫mero total de compara√ß√µes realizadas.
* `swaps`: N√∫mero de trocas efetuadas durante a ordena√ß√£o.
* `elapsedTime`: Tempo total de execu√ß√£o do algoritmo (em segundos).

**M√©todos:**
* `SortMetrics()`: Inicializa todas as m√©tricas com zero.
* `clear()`: Reseta todas as m√©tricas para nova execu√ß√£o experimental.

---
#### `class Sorter<T>`
Classe utilit√°ria gen√©rica que implementa algoritmos cl√°ssicos de ordena√ß√£o.
A implementa√ß√£o manual de algoritmos como Merge Sort e Quick Sort refor√ßa o entendimento de suas propriedades te√≥ricas e pr√°ticas. O uso de templates permite aplicar os mesmos algoritmos a diferentes tipos de dados, enquanto os comparadores externos transformam os m√©todos da classe em fun√ß√µes de ordem superior, possibilitam flexibilidade sem duplica√ß√£o de c√≥digo.

**Principais caracter√≠sticas:**
* Implementa√ß√£o gen√©rica baseada em templates.
* Coleta autom√°tica de m√©tricas de desempenho.
* Suporte a diferentes crit√©rios de ordena√ß√£o via fun√ß√£o comparadora.

**Tipos Internos:**
* `Comparator`: Tipo de fun√ß√£o que compara dois elementos e atualiza `SortMetrics`.

**M√©todos P√∫blicos:**
* `mergeSort(...)`
  * Algoritmo de ordena√ß√£o est√°vel.
  * Complexidade garantida de *O(n log n)*.
  * Adequado para an√°lises comparativas previs√≠veis.

* `quickSort(...)`
  * Algoritmo eficiente na m√©dia (*O(n log n)*), por√©m com pior caso *O(n¬≤)*.
  * √ötil para estudos comparativos de desempenho emp√≠rico.

**M√©todos Privados (Auxiliares):**
* `merge(...)`: Combina subvetores ordenados no Merge Sort.
* `mergeSortRec(...)`: Implementa√ß√£o recursiva do Merge Sort.
* `partition(...)`: Fun√ß√£o de particionamento do Quick Sort.
* `quickSortRec(...)`: Implementa√ß√£o recursiva do Quick Sort.

---
### üìñ Leitura de Texto e An√°lise L√©xico-Sint√°tica

---
### Classe `TextReader`
A classe `TextReader` encapsula o acesso ao arquivo de texto, abstraindo opera√ß√µes de entrada e permitindo que o analisador se concentre exclusivamente na l√≥gica de processamento lingu√≠stico.
Essa classe garante uma leitura controlada e segura do texto, servindo como base para toda a an√°lise subsequente.

**Responsabilidades principais**
* Gerenciar a abertura e fechamento do arquivo de entrada.
* Permitir leitura sequencial linha a linha.
* Manter controle expl√≠cito do n√∫mero da linha atual.

**Atributos:**
* `ifstream file`: fluxo de leitura do arquivo.
* `int currentLine`: contador da linha atual, iniciado em zero.

**M√©todos:**
* `TextReader(const string&)`: abre o arquivo especificado.
* `~TextReader()`: fecha o arquivo se estiver aberto.
* `isOpen()`: verifica se o arquivo foi aberto corretamente.
* `hasNextLine()`: indica se ainda existem linhas a serem lidas.
* `nextLine()`: retorna a pr√≥xima linha do arquivo e incrementa o contador.
* `getCurrentLine()`: retorna o n√∫mero da linha atual.

---
#### Classe `Analyzer`
A classe `Analyzer` concentra toda a l√≥gica de an√°lise l√©xica, sint√°tica e estat√≠stica do texto. Sua arquitetura foi projetada para integrar m√∫ltiplas estruturas de dados cl√°ssicas (listas, filas, pilhas, tabelas hash) em um fluxo √∫nico e coerente, permitindo tanto an√°lise funcional do texto quanto experimenta√ß√£o emp√≠rica de algoritmos.

**Atributos**
* `HashTable<Token> tokens`: todos os tokens do documento.
* `LinkedList<HashTable<Token>> sentenceTokens`: tokens agrupados por senten√ßa.
* `HashTable<Expression> allExpressions`: express√µes detectadas globalmente.
* `LinkedList<HashTable<Expression>> paragraphExpressions`: express√µes por par√°grafo.
* `Queue<Sentence> sentences`: senten√ßas detectadas.
* `Queue<Paragraph> paragraphs`: par√°grafos detectados.
* `Queue<Stack<char>> punctuationBalance`: verifica√ß√£o de balanceamento de pontua√ß√£o por par√°grafo.
* `LinkedList<string> stopWords`: palavras irrelevantes.
* `LinkedList<string> expressions`: express√µes compostas pr√©-definidas.
* `LinkedList<string> abbreviations`: abrevia√ß√µes conhecidas.
* `IntIntMap lengthDist`: distribui√ß√£o do comprimento das palavras.
* `LinkedList<SortMetrics> benchmarkMetrics`: m√©tricas de ordena√ß√£o.
* `LinkedList<int> benchmarkTests`: tamanhos de entrada usados nos testes.

**M√©todos Privados:**
* `loadStopWords(const string &filename)`: L√™ um arquivo contendo palavras irrelevantes (*stop words*) e as armazena em uma lista encadeada ap√≥s normaliza√ß√£o.
  Essas palavras s√£o posteriormente ignoradas na an√°lise estat√≠stica dos tokens.
* `loadExpressions(const string &filename)`: Carrega express√µes compostas previamente definidas, armazenando-as em uma lista encadeada para posterior detec√ß√£o no texto.
* `loadAbbreviations()`: Registra um conjunto fixo de abrevia√ß√µes comuns, utilizado para evitar a segmenta√ß√£o incorreta de senten√ßas ao encontrar pontos finais (seria interessante implementar um arquivo pr√≥prio de abrevia√ß√µes caso continuasse o desenvolvimento do sistema).
* `isSentenceEnd(char c)`: Verifica se um caractere representa um delimitador de fim de senten√ßa (`.`, `!`, `?`).
* `isWordChar(unsigned char c)`: Determina se um caractere pode fazer parte de uma palavra, incluindo letras ASCII e caracteres UTF-8 acentuados.
* `isStopWord(string &word)`: Verifica se uma palavra pertence √† lista de *stop words*, indicando se deve ser desconsiderada na contagem estat√≠stica.
* `checkExpressions(const string &line, int lineNumber, HashTable<Expression> &currentExpressions)`: Procura todas as express√µes conhecidas dentro de uma linha normalizada, registrando suas ocorr√™ncias tanto no contexto global quanto no contexto do par√°grafo atual.
* `utf8ToAscii(unsigned char lead, unsigned char next)`: Converte caracteres UTF-8 acentuados para seus equivalentes ASCII, permitindo tratamento uniforme das palavras.
* `normalizeWord(const string &word)`: Normaliza uma palavra individual, removendo acentos, convertendo para min√∫sculas e preservando h√≠fens em palavras compostas.
* `normalizeLine(const string &line)`: Aplica a normaliza√ß√£o a uma linha inteira, mantendo apenas caracteres relevantes para a an√°lise textual.
* `finalizeSentenceIfPending(...)`: Finaliza uma senten√ßa que ainda n√£o foi encerrada explicitamente por um delimitador. Calcula m√©tricas da senten√ßa (como m√©dia do comprimento das palavras), cria o objeto `Sentence` correspondente e armazena os tokens associados.
* `isDotInNumber(const string &line, size_t i)`: Verifica se um ponto (`.`) est√° entre d√≠gitos num√©ricos, caracterizando um n√∫mero decimal e n√£o o fim de uma senten√ßa.
* `isAbbreviation(const string &word)`: Verifica se a √∫ltima palavra processada corresponde a uma abrevia√ß√£o conhecida, evitando a segmenta√ß√£o indevida de senten√ßas.
* `isOpening(char c)`: Identifica s√≠mbolos de abertura de pontua√ß√£o, como par√™nteses, colchetes, chaves e aspas.
* `isClosing(char c)`: Identifica s√≠mbolos de fechamento de pontua√ß√£o.

* `matches(char open, char close)`: Verifica se um s√≠mbolo de abertura corresponde corretamente a um s√≠mbolo de fechamento.
* `generateLengthDistribution()`: Gera a distribui√ß√£o do comprimento das palavras, agregando as frequ√™ncias em um mapa do tipo `(comprimento ‚Üí ocorr√™ncia)` e ordenando os resultados por comprimento.
* `runSortingBenchmarks()`: Executa testes de desempenho dos algoritmos de ordena√ß√£o implementados (Merge Sort e Quick Sort), utilizando diferentes tamanhos de entrada e crit√©rios de compara√ß√£o. As m√©tricas de compara√ß√µes, trocas e tempo de execu√ß√£o s√£o armazenadas para an√°lise posterior.

**M√©todos P√∫blicos**
* `Analyzer(const string &stopWordsFilename, const string &expressionsFilename)`: Inicializa o analisador carregando os arquivos de *stop words* e de express√µes compostas, al√©m de registrar abrevia√ß√µes conhecidas. Esse pr√©-processamento garante que os crit√©rios lingu√≠sticos estejam dispon√≠veis antes do in√≠cio da an√°lise do texto.
* `analyze(TextReader &reader)`: Executa a an√°lise completa do texto fornecido pelo `TextReader`, populando todas as estruturas internas com os resultados l√©xicos, sint√°ticos e estat√≠sticos.
* `cloneArray(Token* src, int n)`: Cria uma c√≥pia de um vetor de tokens, utilizada nos testes de ordena√ß√£o para garantir condi√ß√µes iniciais id√™nticas.
* `getTokens()`: Retorna a tabela hash com todos os tokens do documento.
* `getSentenceTokens()`: Retorna os tokens agrupados por senten√ßa.
* `getAllExpressions()`: Retorna todas as express√µes detectadas no texto.
* `getParagraphExpressions()`: Retorna as express√µes agrupadas por par√°grafo.
* `getSentences()`: Retorna a fila de senten√ßas identificadas.
* `getParagraphs()`: Retorna a fila de par√°grafos identificados.
* `getPunctuationBalance()`: Retorna o balanceamento de pontua√ß√£o por par√°grafo.
* `getLengthDist()`: Retorna a distribui√ß√£o do comprimento das palavras.
* `getBenchmarkMetrics()`: Retorna as m√©tricas dos testes de ordena√ß√£o.
* `getBenchmarkTests()`: Retorna os tamanhos de entrada utilizados nos benchmarks.

**Fluxo Geral de Execu√ß√£o (`analyze`)**
1. Leitura sequencial do texto.
2. Normaliza√ß√£o e an√°lise caractere a caractere.
3. Identifica√ß√£o de tokens, senten√ßas e par√°grafos.
4. Coleta de m√©tricas lingu√≠sticas e estat√≠sticas.
5. Gera√ß√£o de distribui√ß√µes.
6. Execu√ß√£o de benchmarks de ordena√ß√£o.

---
### üìú Sa√≠das do Algoritmo

---
#### Classe `Report`
A classe `Report` √© respons√°vel pela gera√ß√£o dos relat√≥rios textuais e arquivos CSV a partir dos dados processados pela classe `Analyzer`. Ela centraliza toda a l√≥gica de formata√ß√£o, ordena√ß√£o e apresenta√ß√£o dos resultados, separando claramente a an√°lise dos dados da apresenta√ß√£o dos resultados, o que melhora a modularidade e a organiza√ß√£o do sistema.

**Atributos**
* `Analyzer &analyzer`: Refer√™ncia do Analyzer que cont√©m os dados processados.
* `ostream &out`: Fluxo de sa√≠da usado para redigir o `output.txt`.

**M√©todos Privados**
* `printLine(char c, int n)`: Imprime uma linha composta por um caractere repetido, utilizada para separar visualmente se√ß√µes do relat√≥rio.
* `printTitle(const string &title)`: Imprime um t√≠tulo centralizado, delimitado por linhas decorativas, identificando se√ß√µes principais do relat√≥rio.
* `printPartialResult()`: Gera o relat√≥rio parcial, apresentando os resultados organizados por par√°grafo e senten√ßa, incluindo tokens, express√µes e balanceamento de pontua√ß√£o.
* `printParagraphPartial(...)`: Produz o relat√≥rio detalhado de um √∫nico par√°grafo, exibindo:
  * Tokens ordenados por senten√ßa;
  * Palavras mais frequentes no par√°grafo;
  * Express√µes detectadas;
  * Verifica√ß√£o do balanceamento de s√≠mbolos de pontua√ß√£o;
  * Metadados do par√°grafo.
* `printFullResult()`: Gera o relat√≥rio global consolidado, incluindo:
  * Lista completa de tokens do documento;
  * Tamanho do vocabul√°rio distinto;
  * Palavras mais e menos frequentes;
  * Estat√≠sticas de senten√ßas e par√°grafos;
  * Express√µes detectadas;
  * M√©tricas completas de desempenho dos algoritmos de ordena√ß√£o.
* `printSortedTokenTable(HashTable<Token> &hash, int paragraph, int sentence)`: Exibe uma tabela de tokens ordenados alfabeticamente para uma senten√ßa espec√≠fica.
* `printFrequentlyUsedWords(HashTable<Token> &hash)`: Lista palavras que aparecem tr√™s ou mais vezes dentro de um par√°grafo, destacando termos recorrentes.
* `printSortedExpressionTable(HashTable<Expression> &hash)`: Apresenta as express√µes detectadas, ordenadas alfabeticamente, juntamente com suas frequ√™ncias e linhas de ocorr√™ncia.
* `printMostLeastFrequentWords(HashTable<Token> &hash, int X)`: Exibe as *X* palavras mais frequentes e as *X* menos frequentes do documento.
* `printSentenceStats()`: Apresenta estat√≠sticas por senten√ßa, incluindo:
  * Quantidade de palavras com e sem *stop words*;
  * Comprimento m√©dio das palavras;
  * Comprimento m√©dio das senten√ßas no documento.
* `printParagraphStats()`: Exibe informa√ß√µes estruturais dos par√°grafos, como linha inicial e n√∫mero total de senten√ßas.
* `printLengthDist()`: Mostra a distribui√ß√£o do comprimento das palavras, baseada nos dados agregados pelo analisador.

* `printFullResultSortMetrics(HashTable<Token> &hash)`: Executa novamente os algoritmos de ordena√ß√£o sobre o conjunto completo de tokens e exibe m√©tricas de:
  * Compara√ß√µes;
  * Trocas;
  * Tempo de execu√ß√£o.
* `exportSortMetricsCSV()`: Exporta as m√©tricas de desempenho dos algoritmos de ordena√ß√£o para um arquivo CSV, utilizado posteriormente na an√°lise gr√°fica.
* `exportLengthDist()`: Exporta a distribui√ß√£o do comprimento das palavras para um arquivo CSV.

**M√©todos P√∫blicos**
* `Report(Analyzer &a, ostream &output)`: Inicializa o relat√≥rio com uma refer√™ncia constante ao objeto `Analyzer`, que cont√©m todos os dados processados, e um fluxo de sa√≠da (`ostream`) onde o relat√≥rio ser√° escrito.
* `generate()`: M√©todo principal da classe, respons√°vel por:
  * Gerar o relat√≥rio parcial;
  * Gerar o relat√≥rio completo;
  * Exportar os arquivos CSV auxiliares.

---
#### Fun√ß√£o ``main``
A fun√ß√£o `main` √© o ponto de entrada do sistema, sendo respons√°vel por coordenar a execu√ß√£o das principais classes do projeto.
Ela √© respons√°vel por: Validar os argumentos de linha de comando (nome do arquivo de entrada); Abrir arquivo de entrada; Instanciar `Analyzer` com os arquivos de configura√ß√£o; Executar a an√°lise textual; Criar objeto `Report` e gerar relat√≥rio final; Encerrar o programa com mensagem de sucesso.

---
### üìä Scripts em Python
Os scripts em Python s√£o utilizados para p√≥s-processamento e visualiza√ß√£o gr√°fica dos dados gerados pelo sistema em C++. Essa abordagem permite separar a an√°lise algor√≠tmica da visualiza√ß√£o, facilitando experimenta√ß√£o e reutiliza√ß√£o dos dados.

---
#### `plot_utils.py`
M√≥dulo utilit√°rio que centraliza a l√≥gica de salvamento de figuras do Matplotlib.
**Fun√ß√£o:**
* `save_figure(output_dir, filename)`: Garante a exist√™ncia do diret√≥rio de sa√≠da, salva a figura com alta resolu√ß√£o e libera recursos de mem√≥ria.

---
#### `plot_length_dist.py`
Script respons√°vel por gerar o gr√°fico de "distribui√ß√£o do comprimento das palavras".
**Funcionalidades:**
* Leitura do arquivo `length_dist.csv`;
* Ordena√ß√£o dos dados por comprimento da palavra;
* Plotagem de um gr√°fico de barras representando frequ√™ncia por tamanho;
* Salvamento autom√°tico da figura no diret√≥rio de sa√≠da.

Esse gr√°fico permite analisar padr√µes lingu√≠sticos relacionados ao tamanho m√©dio das palavras no texto.

---
#### `plot_sort_metrics.py`
Script respons√°vel por gerar gr√°ficos de **desempenho dos algoritmos de ordena√ß√£o**.
**Funcionalidades:**
* Leitura do arquivo `sort_metrics.csv`;
* Agrupamento dos dados por algoritmo e crit√©rio de ordena√ß√£o;
* Gera√ß√£o de gr√°ficos de linha para:
  * Tempo de execu√ß√£o;
  * N√∫mero de compara√ß√µes;
  * N√∫mero de trocas;
* Compara√ß√£o visual entre Merge Sort e Quick Sort.

Esses gr√°ficos permitem avaliar empiricamente o comportamento dos algoritmos em diferentes tamanhos de entrada.

---
## üßÆ Resultados
Essa se√ß√£o apresenta os principais resultados obtidos a partir da execu√ß√£o do Analisador L√©xico-Sint√°tico, considerando o arquivo de entrada de exemplo ``data/Semana_Machado_Assis.txt``.
### Exemplo de sa√≠da do analisador
O arquivo `output/output.txt` cont√©m o relat√≥rio textual completo gerado pelo sistema. Segue uma parte da sa√≠da para o texto ``data/Semana_Machado_Assis.txt``:
```
======================================================================================================================================================
=>                                                                ### START PROCESS ###

======================================================================================================================================================
======================================================================================================================================================
=>                                                                ### PARTIAL RESULT ###

======================================================================================================================================================
______________________________________________________________________________________________________________________________________________________
WORD                     FREQUENCY      PARAGRAPH      SENTENCE       LINE           POSITIONS
------------------------------------------------------------------------------------------------------------------------------------------------------
assis                    1              1              1              1              9 
completa                 1              1              1              1              5 
machado                  1              1              1              1              7 
obra                     1              1              1              1              4 
semana                   1              1              1              1              2 
texto-fonte              1              1              1              1              3 
______________________________________________________________________________________________________________________________________________________
=> Number of words with stop words: 9                                                           => Number of words without stop words: 6
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
=> Balanced symbols: YES
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
=> Beginning paragraph in line: 1  Number of sentences: 1
______________________________________________________________________________________________________________________________________________________

...

______________________________________________________________________________________________________________________________________________________
DISTINCT VOCABULARY SIZE: 24721 WORDS
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
TOP 10 MOST FREQUENT WORDS
WORD                FREQUENCY   
------------------------------------------------------------------------------------------------------------------------------------------------------
ainda               629
tudo                563
pode                548
outro               475
homem               471
todos               455
assim               454
outra               437
aqui                421
outros              419
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
TOP 10 LEAST FREQUENT WORDS
WORD                FREQUENCY   
------------------------------------------------------------------------------------------------------------------------------------------------------
repetirei           1
desazo              1
refuta              1
esfriaras           1
agasalhar           1
di-lo               1
personalidades      1
apaga               1
restante            1
birmingham          1
______________________________________________________________________________________________________________________________________________________

...
```
**Exemplo de sa√≠da de output.txt**

### Distribui√ß√£o por comprimento das palavras
A distribui√ß√£o do comprimento das palavras foi obtida a partir do arquivo ``length_dist.csv`` e visualizada no gr√°fico apresentado abaixo, armazenado no diret√≥rio `assets`.
![Distribui√ß√£o por comprimento](assets/length_distribution.png)
**Distribui√ß√£o por comprimento para `data/Semana_Machado_Assis.txt`**
√â not√°vel que a maioria das palavras do texto est√£o com comprimento em torno de 5 caract√©res, o que √© aceit√°vel para lingua portuguesa.
### Desempenho dos algoritmos de ordena√ß√£o
O desempenho dos algoritmos MergeSort e QuickSort foi avaliado considerando dois crit√©rios de ordena√ß√£o:
* Ordem alfab√©tica;
* Ordem por frequ√™ncia.
As m√©tricas analisadas foram:
* Tempo de execu√ß√£o (em segundos);
* N√∫mero de compara√ß√µes;
* N√∫mero de trocas (swaps).
Os resultados est√£o novamente no diret√≥rio `assets`.
#### Tempo de execu√ß√£o
![Performance tempo](assets/sort_performance_time.png)
Percebe-se que o tempo do QuickSort por frequ√™ncia tem um aumento brusco para $n\approx 25000$, provavelmente resultado de pivos ruins de parti√ß√£o.
#### Compara√ß√µes e trocas
![Performance compara√ß√µes](assets/sort_performance_comparisons.png)
Nas compara√ß√µes, nota-se que o QuickSort por frequ√™ncia novamente tem um estouro de compara√ß√µes em rela√ß√£o aos outros algoritmos, isso pode ser devido a grande quantidade de chaves com mesmo valor (palavras com a mesma frequ√™ncia), o que causa esse comportamento descontrolado do QuickSort.
![Performance trocas](assets/sort_performance_swaps.png)
No caso de trocas, ocorre o efeito contr√°rio, pois devido a v√°rias palavras terem a mesma frequ√™ncia, o algoritmo do QuickSort consegue melhor desempenho gra√ßas a sua adaptabilidade (capacidade de melhorar o desempenho caso a estrutura j√° esteja parcialmente ordenada).
### Compara√ß√£o com an√°lise assint√≥tica
A an√°lise de desempenho dos algoritmos de ordena√ß√£o considerou tanto os tempos de execu√ß√£o observados experimentalmente quanto o comportamento assint√≥tico esperado para cada m√©todo. Conforme definido no arquivo de `pr√°tica.pdf`, o custo te√≥rico pode ser modelado por:
$$T(n)=c_1\times nlog(n)+c_2\times n+O(1)$$
onde as constantes $c_1$ e $c_2$ dependem das opera√ß√µes elementares realizadas, como compara√ß√µes e trocas, al√©m das caracter√≠sticas do hardware utilizado.
A fim de estimar o valor dessas constantes para comparar com o valores obtidos da execu√ß√£o, aproximou-se a formula para:
$$T(n)\approx c_1\times log_2(n)$$
pois $O(1)$ √© desprez√≠vel para grandes valore de $n$ e as compara√ß√µes s√£o o maior custo. Ao utilizar o ponto experimental de $n\approx 25000$, obt√©m-se uma estimativa para $c_1$:
MergeSort (ordem alfab√©tica, $n=24721$)
* $n=24721$
* $T_{real} = 0.504115 \; s$ 
* $log_2(24721)\approx 14.6$
$$n\;log_2\;n\approx 24721\times 14.6 \approx 360927$$
Logo:
$$c_1 \approx \frac{T{real}}{n\; log\; n} = \frac{0.504115}{360927} \approx \boxed{1.4\times 10^{-6}}$$
Usando esse $c_1$ para o MergeSort alfab√©tico:

|$n$|$n\; log\; n$|$T_{te√≥rico}(s)$|$T_{real}(s)$|
|-|-|-|-|
|$1000$|$9966$|$0.014$|$0.016$|
|$5000$|$61440$|$0.086$|$0.096$|
|$10000$|$132880$|$0.186$|$0.197$|
|$24721$|$360927$|$0.505$|$0.504$|

**Tabela de aproxima√ß√£o do tempo assint√≥tico do MergeSort alfab√©tico.**
A partir dessa tabela, nota-se que os valores de $T_{te√≥rico}(s)$ se aproximam consideravelmente do tempo $T_{real}(s)$.

J√° para o QuickSort por frequ√™ncia, encontra essa rela√ß√£o:
|$n$|Compara√ß√µes|
|-|-|
|$5000$|$3683894$|
|$10000$|$14768827$|
|$24721$|$91232211$|

E considerando o crescimento:
* $5k \to 10k \approx 4.0$
* $10k \to 24k \approx 6.177$

Encontra-se um comportamento mais pr√≥ximo de $n^2$ do que $n\;log\;n$. Como havia mencionado anteriormente, isso pode ser causado por muitas frequ√™ncias iguais e o QuickSort implementado n√£o usa mediana de tr√™s, causando um $T(n)\approx c\times n^2$, mesmo que o tempo de execu√ß√£o seja aceit√°vel, o n√∫mero de compara√ß√µes explode, confirmando o pior caso te√≥rico.

## üèÅ Conclus√£o
A execu√ß√£o do Analisador L√©xico-Sint√°tico sobre o arquivo `data/Semana_Machado_Assis.txt` permitiu avaliar, de forma pr√°tica e quantitativa, tanto a qualidade da an√°lise textual quanto o desempenho dos algoritmos de ordena√ß√£o aplicados ao processamento dos dados l√©xicos. O relat√≥rio gerado em `output/output.txt` evidencia a correta extra√ß√£o de informa√ß√µes como frequ√™ncia de palavras, distribui√ß√£o por senten√ßas e par√°grafos, vocabul√°rio distinto e verifica√ß√£o de balanceamento de s√≠mbolos, confirmando a efic√°cia das estruturas de dados desenvolvidas.
O elevado tamanho do vocabul√°rio distinto (24 721 palavras) confirma a riqueza lexical do texto analisado, caracter√≠stica esperada de uma obra liter√°ria extensa. A distribui√ß√£o por comprimento das palavras, obtida a partir de `length_dist.csv`, apresentou concentra√ß√£o em torno de palavras com aproximadamente cinco caracteres, com decaimento gradual para comprimentos maiores, comportamento coerente com a l√≠ngua portuguesa e indicativo do correto funcionamento do processo de tokeniza√ß√£o e agrupamento estat√≠stico.
Quanto ao desempenho dos algoritmos de ordena√ß√£o, os resultados experimentais foram analisados √† luz do modelo assint√≥tico, permitindo comparar os tempos medidos com os tempos te√≥ricos estimados. Para o MergeSort, observou-se forte ader√™ncia ao comportamento ($O(n \log n)$) em todos os cen√°rios, com crescimento est√°vel do tempo de execu√ß√£o e boa aproxima√ß√£o entre valores emp√≠ricos e te√≥ricos, evidenciando sua previsibilidade e robustez.
Em contrapartida, o QuickSort apresentou desempenho dependente do crit√©rio de ordena√ß√£o. Na ordena√ß√£o alfab√©tica, o comportamento manteve-se pr√≥ximo do caso m√©dio ($O(n \log n)$); entretanto, na ordena√ß√£o por frequ√™ncia, verificou-se aumento significativo no n√∫mero de compara√ß√µes para grandes valores de ($n$), aproximando-se do pior caso ($O(n^2)$). Esse efeito est√° associado √† alta incid√™ncia de frequ√™ncias repetidas e √† aus√™ncia da estrat√©gia de mediana de tr√™s, resultando em parti√ß√µes desbalanceadas.
Dessa forma, os resultados pr√°ticos corroboram a an√°lise assint√≥tica cl√°ssica: o MergeSort mostrou-se mais adequado ao contexto da an√°lise textual, caracterizado por grandes volumes de dados e muitas chaves repetidas, enquanto o QuickSort apresentou melhor desempenho em situa√ß√µes favor√°veis, por√©m com maior sensibilidade √† distribui√ß√£o dos dados. O trabalho, portanto, atingiu seus objetivos ao integrar an√°lise l√©xico-sint√°tica, avalia√ß√£o experimental de algoritmos e compara√ß√£o com modelos te√≥ricos de tempo de execu√ß√£o.
## üîß Configura√ß√£o do Ambiente

Para garantir a correta compila√ß√£o e execu√ß√£o do Analisador L√©xico-Sint√°tico, √© necess√°rio que o ambiente de desenvolvimento esteja configurado conforme as especifica√ß√µes a seguir.

* **Sistema Operacional**:
  Linux Ubuntu 22.04 ou 24.04 LTS (recomendado).
  O projeto tamb√©m pode ser compilado em Windows, desde que o compilador e as ferramentas estejam corretamente configurados.

* **Compilador**:
  GCC vers√£o 13 ou superior, com suporte ao padr√£o **C++17**.

Para verificar a vers√£o instalada do compilador, utilize:

```bash
g++ --version
```

Caso seja necess√°rio instalar ou atualizar o compilador e as ferramentas essenciais de build no Ubuntu, execute:

```bash
sudo apt update
sudo apt install build-essential g++
```

* **Bibliotecas**:
  O projeto utiliza exclusivamente a biblioteca padr√£o da linguagem C++. N√£o h√° depend√™ncias externas de terceiros.

* **Python (opcional)**:
  Python 3 e o `venv` s√£o utilizados apenas para a gera√ß√£o dos gr√°ficos de an√°lise de desempenho e distribui√ß√£o por comprimento, atrav√©s dos scripts localizados no diret√≥rio `utils/`.

---

## üíª Como Compilar e Executar

O projeto utiliza um **Makefile** para padronizar os processos de compila√ß√£o, execu√ß√£o e gera√ß√£o de gr√°ficos, garantindo reprodutibilidade dos experimentos.
### Clone o Reposit√≥rio
``` bash
git clone https://github.com/JV-NC/LexicalSyntacticAnalyzer.git
```

### Estrutura de Entrada

* **Arquivo de entrada textual**:
  Por padr√£o, o sistema utiliza o arquivo:

  ```
  data/DomCasmurro.txt
  ```

  O arquivo de entrada pode ser alterado via vari√°vel `INPUT` no momento da execu√ß√£o, sem necessidade de modificar o c√≥digo-fonte.

Exemplo:

```bash
make run INPUT=data/Semana_Machado_Assis.txt
```

---

### Compila√ß√£o

Para compilar o projeto, navegue at√© o diret√≥rio raiz e execute:

```bash
make clean
make
```

* `make clean`: remove arquivos objeto (`.o`), diret√≥rios de build e sa√≠das anteriores, garantindo uma compila√ß√£o limpa.
* `make`: compila todos os arquivos `.cpp` presentes em `src/` e gera o execut√°vel no diret√≥rio `bin/`.

---

### Execu√ß√£o

Ap√≥s a compila√ß√£o, o analisador pode ser executado com:

```bash
make run
```

Esse comando ir√°:

* Executar o analisador l√©xico-sint√°tico sobre o arquivo definido em `INPUT`;
* Processar o texto, realizando tokeniza√ß√£o, an√°lise sint√°tica b√°sica e coleta de estat√≠sticas;
* Gerar o relat√≥rio textual no diret√≥rio `output/`, incluindo o arquivo `output.txt`.

---

### Gera√ß√£o dos Gr√°ficos

Para gerar os gr√°ficos de **distribui√ß√£o por comprimento** e **desempenho dos algoritmos de ordena√ß√£o**, execute:

```bash
make plots
```

Esse comando:

* Executa o analisador (caso ainda n√£o tenha sido executado);
* Utiliza os scripts Python em `utils/` para processar os arquivos CSV gerados;
* Salva os gr√°ficos no diret√≥rio `output/`.

---

### Execu√ß√£o Completa

Para realizar todo o processo automaticamente (limpeza, compila√ß√£o, execu√ß√£o e gera√ß√£o de gr√°ficos), utilize:

```bash
make full
```

Esse fluxo garante a completa reprodutibilidade dos resultados apresentados neste relat√≥rio.

---
## üñ•Ô∏è Hardware Utilizado

Os experimentos apresentados na Se√ß√£o de Resultados foram executados em um ambiente controlado, utilizando um √∫nico sistema computacional, com o objetivo de garantir consist√™ncia e reprodutibilidade nas medi√ß√µes de desempenho dos algoritmos analisados. Todas as medi√ß√µes de tempo, n√∫mero de compara√ß√µes e n√∫mero de trocas foram obtidas a partir da execu√ß√£o local do analisador l√©xico-sint√°tico, sem concorr√™ncia de outros processos computacionalmente intensivos, buscando minimizar interfer√™ncias externas nos resultados experimentais.

> **Tabela 2 ‚Äî Especifica√ß√µes do hardware utilizado nos experimentos.**

|Componente|Especifica√ß√£o|
|-|-|
|Processador|AMD Ryzen 7 5700G|
|Arquitetura|x86_64|
|Frequ√™ncia Base|3.80GHz|
|Mem√≥ria RAM|16GB DDR4 3200MHz|
|Sistema Operacional|Windows 10 22H2|
|Compilador|GCC 6.3.0|

----
## üë§ Autoria

<div align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=22&pause=1000&color=6366F1&center=true&vCenter=true&width=400&lines=Autor+do+Projeto;Aproveitamento+Estudos" alt="Typing SVG" />
</div>

### üöÄ Autor

<table align="center">
  <tr>
    <td align="center">
      <a href="https://github.com/JV-NC">
        <img src="https://github.com/JV-NC.png" width="120px;" alt="Jo√£o Vitor Neves"/><br>
        <sub><b>Jo√£o Vitor Neves</b></sub>
      </a><br><br>
      <a href="https://github.com/joaovitor3105" title="GitHub">
        <img src="https://img.shields.io/github/followers/JV-NC?label=Seguidores&style=social&logo=github" alt="GitHub - Jo√£o Vitor">
      </a>
    </td>
  </tr>
</table>

### üìä Estat√≠sticas do Reposit√≥rio

<div align="center">
  <img src="https://img.shields.io/github/commit-activity/t/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=git&label=Total%20Commits" alt="Total de commits">
  <img src="https://img.shields.io/github/languages/top/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=cplusplus" alt="Linguagem principal">
  <img src="https://img.shields.io/github/issues-pr-closed/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=github&label=PRs%20Fechados" alt="PRs Fechados">
  <img src="https://img.shields.io/github/license/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=open-source-initiative&label=Licen√ßa" alt="Licen√ßa do projeto">
</div>

---
## üìö Refer√™ncias

Esta se√ß√£o re√∫ne os principais materiais te√≥ricos, t√©cnicos e documentais utilizados como base para o desenvolvimento do Analisador L√©xico-Sint√°tico, bem como para a an√°lise de desempenho dos algoritmos de ordena√ß√£o e a interpreta√ß√£o dos resultados experimentais apresentados neste trabalho.
