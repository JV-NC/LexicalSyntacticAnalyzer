<h1 align='center'>
         ğŸ” Analisador LÃ©xico-SintÃ¡tico
</h1>

<p align="center">
  <img src="https://img.shields.io/badge/C%2B%2B-00599C?style=for-the-badge&logo=c%2B%2B&logoColor=white" alt="C++">
  <img src="https://img.shields.io/badge/GCC-orange?style=for-the-badge&logo=gnu-compiler-collection&logoColor=white" alt="GCC">
  <img src="https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54" alt="Python">
  <img src="https://img.shields.io/badge/Ubuntu-E95420?style=for-the-badge&logo=ubuntu&logoColor=white" alt="Ubuntu">
  <img src="https://img.shields.io/badge/Visual_Studio_Code-007ACC?style=for-the-badge&logo=visual-studio-code&logoColor=white" alt="Visual Studio Code">
  <img src="https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white" alt="Git">
  <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub">
  
<img src="https://img.shields.io/badge/-Makefile-red?style=for-the-badge&logo=gnu-make&logoColor=white" alt="Makefile">
<img src="https://img.shields.io/badge/doxygen-2C4AA8?style=for-the-badge&logo=doxygen&logoColor=white" alt="Doxygen"></p>
<div align='center'>
LaboratÃ³rio de Algoritmos e Estruturas de Dados I <br>
Engenharia de ComputaÃ§Ã£o <br>
Prof. Michel Pires da Silva <br>
CEFET-MG Campus V <br>
2026/1  
</div>

---
<details>
<summary><h2>ğŸ“‹ Ãndice</h2></summary>

* [ğŸ” Analisador LÃ©xico-SintÃ¡tico](#-analisador-lÃ©xico-sintÃ¡tico)
  * [ğŸ“ IntroduÃ§Ã£o](#-introduÃ§Ã£o)
  * [ğŸ¯ Objetivos](#objetivos)
    * [Objetivo Geral](#objetivo-geral)
    * [Objetivos EspecÃ­ficos](#objetivos-especÃ­ficos)
  * [ğŸ“š FundamentaÃ§Ã£o TeÃ³rica](#-fundamentaÃ§Ã£o-teÃ³rica)
    * [AnÃ¡lise LÃ©xica](#anÃ¡lise-lÃ©xica-scanner)
    * [AnÃ¡lise SintÃ¡tica](#anÃ¡lise-sintÃ¡tica-parser)
    * [Pilhas](#pilhas-stacks)
    * [Filas](#filas-queues)
    * [Listas Encadeadas](#listas-encadeadas-linked-lists)
    * [Tabelas Hash](#tabelas-hash-hash-tables)
    * [Mapas Associativos](#mapas-associativos)
    * [Algoritmos de OrdenaÃ§Ã£o](#algoritmos-de-ordenaÃ§Ã£o)
      * [MergeSort](#merge-sort)
      * [QuickSort](#quick-sort)
    * [MergeSort X QuickSort](#merge-sort-x-quick-sort)
  * [ğŸ”¬ Modelagem da AplicaÃ§Ã£o](#-modelagem-da-aplicaÃ§Ã£o)
    * [Fluxo de ExecuÃ§Ã£o do Sistema](#fluxo-de-execuÃ§Ã£o-do-sistema)
    * [ğŸ“Š Estruturas de Dados](#-estruturas-de-dados)
    * [ğŸš€ OtimizaÃ§Ãµes Propostas](#-otimizaÃ§Ãµes-propostas)
    * [PrÃ©-processamento Textual](#prÃ©-processamento-textual)
    * [Uso de Hashing](#uso-de-hashing)
    * [AnÃ¡lise EmpÃ­rica de OrdenaÃ§Ã£o](#anÃ¡lise-empÃ­rica-de-ordenaÃ§Ã£o)
  * [ğŸ“ Metodologia](#-metodologia)
    * [ğŸ“ Estrutura do Projeto](#-estrutura-do-projeto)
    * [ğŸ“ Arquivos e DiretÃ³rios](#-arquivos-e-diretÃ³rios)
    * [ğŸ“š Bibliotecas](#-bibliotecas)
    * [ğŸ“Œ Bibliotecas do Projeto (Headers Customizados)](#-bibliotecas-do-projeto-headers-customizados)
    * [ğŸ“Œ Bibliotecas PadrÃ£o do C++](#-bibliotecas-padrÃ£o-do-c)
  * [âš™ï¸ Estruturas de Dados e ImplementaÃ§Ã£o do Sistema](#ï¸-estruturas-de-dados-e-implementaÃ§Ã£o-do-sistema)
    * [ğŸ§± Estruturas Base do Sistema](#-estruturas-base-do-sistema)
      * [`struct Occurrence`](#struct-occurrence)
      * [`class Token`](#class-token)
      * [`class Sentence`](#class-sentence)
      * [`class Paragraph`](#class-paragraph)
      * [`class Expression`](#class-expression)
    * [ğŸ§± Estruturas de Dados GenÃ©ricas](#-estruturas-de-dados-genÃ©ricas)
      * [`struct Node<T>`](#struct-nodet)
      * [`class Stack<T>`](#class-stackt)
      * [`class Queue<T>`](#class-queuet)
      * [`class LinkedList<T>`](#class-linkedlistt)
      * [`class HashTable<T>`](#class-hashtablet)
    * [ğŸ—ºï¸ Estruturas Auxiliares de Mapeamento](#ï¸-estruturas-auxiliares-de-mapeamento)
      * [`struct MapEntry`](#struct-mapentry)
      * [`class IntIntMap`](#class-intintmap)
    * [ğŸ”€ Estruturas de ComparaÃ§Ã£o e OrdenaÃ§Ã£o](#-estruturas-de-comparaÃ§Ã£o-e-ordenaÃ§Ã£o)
      * [FunÃ§Ãµes de ComparaÃ§Ã£o para `Expression`](#funÃ§Ãµes-de-comparaÃ§Ã£o-para-expression-expressioncomparatorshpp)
      * [FunÃ§Ãµes de ComparaÃ§Ã£o para `Token`](#funÃ§Ãµes-de-comparaÃ§Ã£o-para-token-tokencomparatorshpp)
      * [FunÃ§Ãµes de ComparaÃ§Ã£o para `MapEntry`](#funÃ§Ãµes-de-comparaÃ§Ã£o-para-mapentry-mapentrycomparatorshpp)
      * [`struct SortMetrics`](#struct-sortmetrics)
      * [`class Sorter<T>`](#class-sortert)
    * [ğŸ“– Leitura de Texto e AnÃ¡lise LÃ©xico-SintÃ¡tica](#-leitura-de-texto-e-anÃ¡lise-lÃ©xico-sintÃ¡tica)
      * [`class TextReader`](#class-textreader)
      * [ `class Analyzer`](#class-analyzer)
    * [ğŸ“œ SaÃ­das do Algoritmo](#-saÃ­das-do-algoritmo)
      * [`class Report`](#class-report)
      * [FunÃ§Ã£o `main`](#funÃ§Ã£o-main)
    * [ğŸ“Š Scripts em Python](#-scripts-em-python)
      * [`plot_utils.py`](#plot_utilspy)
      * [`plot_length_dist.py`](#plot_length_distpy)
      * [`plot_sort_metrics.py`](#plot_sort_metricspy)
  * [ğŸ§® Resultados](#-resultados)
    * [Exemplo de SaÃ­da do Analisador](#exemplo-de-saÃ­da-do-analisador)
    * [DistribuiÃ§Ã£o por comprimento das palavras](#distribuiÃ§Ã£o-por-comprimento-das-palavras)
    * [Desempenho dos algoritmos de ordenaÃ§Ã£o](#desempenho-dos-algoritmos-de-ordenaÃ§Ã£o)
      * [Tempo de ExecuÃ§Ã£o](#tempo-de-execuÃ§Ã£o)
      * [ComparaÃ§Ãµes e trocas](#comparaÃ§Ãµes-e-trocas)
    * [ComparaÃ§Ã£o com anÃ¡lise assintÃ³tica](#comparaÃ§Ã£o-com-anÃ¡lise-assintÃ³tica)
  * [ğŸ ConclusÃ£o](#-conclusÃ£o)
  * [ğŸ–¥ï¸ Hardware Utilizado](#ï¸-hardware-utilizado)
  * [ğŸ”§ ConfiguraÃ§Ã£o do Ambiente](#-configuraÃ§Ã£o-do-ambiente)
  * [ğŸ’» Como Compilar e Executar](#-como-compilar-e-executar)
    * [Clone do RepositÃ³rio](#clone-do-repositÃ³rio)
    * [PrÃ©-requisitos](#prÃ©-requisitos)
    * [CompilaÃ§Ã£o](#compilaÃ§Ã£o)
    * [ExecuÃ§Ã£o](#execuÃ§Ã£o)
    * [GeraÃ§Ã£o de GrÃ¡ficos](#geraÃ§Ã£o-de-grÃ¡ficos)
  * [ğŸ‘¤ Autoria](#-autoria)
    * [Autor do Projeto](#autor-do-projeto)
    * [EstatÃ­sticas do RepositÃ³rio](#estatÃ­sticas-do-repositÃ³rio)
  * [ğŸ“š ReferÃªncias](#-referÃªncias)
</details>

---
## ğŸ“ IntroduÃ§Ã£o

Este projeto foi desenvolvido como trabalho de aproveitamento da disciplina de LaboratÃ³rio de AlgorÃ­tmos e Estruturas de Dados I (LAEDI), sob a orientaÃ§Ã£o do professor [Michel Pires Silva](https://github.com/mpiress). Este trabalho tem como objetivo principal o desenvolvimento de um sistema denominado Analisador LÃ©xico-SintÃ¡tico (LSA), capaz de avaliar diferentes mÃ©tricas associadas Ã  qualidade textual.
O LSA Ã© responsÃ¡vel por processar um texto de entrada e extrair informaÃ§Ãµes estatÃ­sticas, estruturais e semÃ¢nticas sobre seu conteÃºdo. A partir da leitura sequencial do texto, o sistema identifica palavras (*tokens*), expressÃµes compostas, sentenÃ§as, parÃ¡grafos e sÃ­mbolos de pontuaÃ§Ã£o, organizando esses dados por meio de estruturas de dados implementadas manualmente, como:
* Listas encadeadas (LinkedList).
* Pilhas (Stack).
* Filas (Queue).
* Tabelas hash (HashTable).

Todas as estruturas foram implementadas durante o projeto, sem o uso de bibliotecas prontas da Standard Template Library (STL), respeitando as restriÃ§Ãµes e objetivos didÃ¡ticos da disciplina aproveitada.
Durante o processamento do texto, o sistema realiza diversas tarefas, entre elas:
* Contagem da frequÃªncia de palavras e expressÃµes.
* IdentificaÃ§Ã£o de palavras irrelevantes (*stop words*).
* Registro detalhado das ocorrÃªncias dos *tokens* (parÃ¡grafo, sentenÃ§a, linha e posiÃ§Ã£o).
* VerificaÃ§Ã£o do balanceamento de sÃ­mbolos de pontuaÃ§Ã£o.
* GeraÃ§Ã£o de estatÃ­sticas por sentenÃ§a e por parÃ¡grafo.
* ConstruÃ§Ã£o da distribuiÃ§Ã£o de comprimento das palavras.

AlÃ©m disso, o sistema incorpora anÃ¡lise de desempenho de algoritmos de ordenaÃ§Ã£o, comparando *MergeSort* e *QuickSort* usando diferentes comparadores e variÃ¡veis de ordenaÃ§Ã£o. Para cada algoritmo, sÃ£o coletadas as seguintes mÃ©tricas:
* NÃºmero de comparaÃ§Ãµes
* NÃºmero de trocas
* Tempo de execuÃ§Ã£o

Esses dados e a distribuiÃ§Ã£o por comprimento sÃ£o exportados no formato `.csv` e posteriormente utilizados para plotagem de grÃ¡ficos com scripts em Python.
Como parte da automaÃ§Ã£o do projeto, foi desenvolvido pensando na execuÃ§Ã£o em *pipeline* a partir do Makefile, tendo a seguinte ordem:
1. CompilaÃ§Ã£o e criaÃ§Ã£o dos objetos (.o) do cÃ³digo em C++.
2. ExecuÃ§Ã£o do analisador sobre o texto de entrada.
3. GeraÃ§Ã£o do ``output.txt`` e dos arquivos `.csv`.
4. Plotagem dos grÃ¡ficos utilizando ``pandas`` e ``matplolib`` em Python.

Os textos utilizados para testar e analisar o projeto foram Dom Casmurro e a Semana Machado de Assis, ambas obras de Machado de Assis, oferecidas inicialmente para o trabalho de aproveitamento. Estes materiais foram selecionados devido ao tamanho textual e riqueza linguÃ­stica, possibilitando executar o sistema em um cenÃ¡rio realista e desafiador para anÃ¡lise.
Com este trabalho, busca-se consolidar os conceitos fundamentais da disciplina de LAEDI, demonstrando o conhecimento do autor sobre a disciplina em uma aplicaÃ§Ã£o real, mensurÃ¡vel e extensÃ­vel.

---
## ğŸ¯Objetivos

### Objetivo Geral
O objetivo geral deste trabalho Ã© projetar e implementar um Analisador LÃ©xico-SintÃ¡tico para processamento de texto em linguagem natural, capaz de identificar tokens, sentenÃ§as, parÃ¡grafos, expressÃµes e padrÃ³es sintÃ¡ticos, utilizando estruturas de dados fundamentais e algoritmos de ordenaÃ§Ã£o, com foco em corretude, elegÃ¢ncia e desempenho computacional.
Busca-se aplicar, de forma prÃ¡tica os conceitos de Algoritmos e Estruturas de Dados I, integrando anÃ¡lise lÃ©xica, controle sintÃ¡tico bÃ¡sico (balanceamento de sÃ­mbolos) e geraÃ§Ã£o de estatÃ­sticas textuais relevantes, podendo ser relacionado a disciplinas posteriores do curso, como Linguagens de ProgramaÃ§Ã£o, Linguagens Formais e AutÃ´matos e Compiladores.
#### MÃ©tricas de Desempenho
A avaliaÃ§Ã£o do sistema desenvolvido considera os seguintes critÃ©rios:
* **Tempo de ExecuÃ§Ã£o:**
  Tempo necessÃ¡rio para realizar a anÃ¡lise completa do texto e executar os algoritmos de ordenaÃ§Ã£o, medido em segundos.
* **Uso de Estruturas de Dados:**
  AplicaÃ§Ã£o correta e elegante de estruturas de dados como pilhas, filas, listas encadeadas e tabelas hash, respeitando suas caracterÃ­sticas.
* **Complexidade assintÃ³tica:**
  AnÃ¡lise do comportamento assintÃ³tico das principais operaÃ§Ãµes, como inserÃ§Ãµes em tabelas hash, percursos em listas encadeadas e algoritmos de ordenaÃ§Ã£o (*MergeSort* e *QuickSort*).
* **Qualidade da AnÃ¡lise Textual:**
  Capacidade do sistema em extrair informaÃ§Ãµes lÃ©xicas e estruturais relevantes, como frequÃªncia de tokens, distribuiÃ§Ã£o de comprimento das palavras e identificaÃ§Ã£o de expressÃµes.

### Objetivos EspecÃ­ficos
* **Implementar um analisador lÃ©xico para textos em linguagem natural:**
  Identificar e normalizar tokens (palavras), removendo *stopwords*, tratando abreviaÃ§Ãµes, nÃºmeros e caracteres especiais, alÃ©m de registrar ocorrÃªncias detalhadas (parÃ¡grafo, sentenÃ§a, linha e posiÃ§Ã£o).
* **Realizar anÃ¡lise sintÃ¡tica bÃ¡sica do texto:**
  Verificar o balanceamento de sÃ­mbolos de pontuaÃ§Ã£o (parÃªnteses, colchetes, chaves, aspas), utilizando estruturas de pilha, identificando inconsistÃªncias sintÃ¡ticas.
* **Organizar informaÃ§Ãµes textuais por sentenÃ§as e parÃ¡grafos:**
  Estruturar os dados analisados em nÃ­veis hierÃ¡rquicos (texto â†’ parÃ¡grafos â†’ sentenÃ§as), armazenando estatÃ­sticas como nÃºmero de palavras, palavras sem *stopwords* e comprimento mÃ©dio, alÃ©m de facilitar a gestÃ£o das estruturas de dados globais, como a listas de tabela hash para tokens e expressÃµes.
* **Detectar e contabilizar expressÃµes prÃ©-definidas:**
  Identificar expressÃµes compostas ao longo do texto, registrando sua frequÃªncia e linhas de ocorrÃªncia.
* **Construir distribuiÃ§Ãµes estatÃ­sticas do texto:**
  Gerar a distribuiÃ§Ã£o de comprimento das palavras, utilizando uma estrutura de mapa (`IntIntMap`) implementada sem dependÃªncia da STL.
* **Aplicar e comparar algoritmos de ordenaÃ§Ã£o:**
  Implementar e avaliar *MergeSort* e *QuickSort* para diferentes critÃ©rios de ordenaÃ§Ã£o (ordem alfabÃ©tica e frequÃªncia), coletando mÃ©tricas de comparaÃ§Ãµes, trocas e tempo de execuÃ§Ã£o.
* **Exportar dados para anÃ¡lise externa:**
  Gerar arquivos `.csv` contendo mÃ©tricas de ordenaÃ§Ã£o e distribuiÃ§Ãµes estatÃ­sticas, possibilitando a visualizaÃ§Ã£o grÃ¡fica por meio de scripts em Python.
* **Documentar o projeto com Doxygen:**
  Produzir documentaÃ§Ã£o tÃ©cnica completa do cÃ³digo-fonte, incluindo descriÃ§Ã£o de classes, mÃ©todos, parÃ¢metros e estruturas de dados utilizadas.

---
## ğŸ“š FundamentaÃ§Ã£o TeÃ³rica
O desenvolvimento de um sistema de anÃ¡lise textual eficiente exige a aplicaÃ§Ã£o integrada de conceitos clÃ¡ssicos de estruturas de dados, algoritmos de ordenaÃ§Ã£o e processamento de texto. Este trabalho fundamenta-se nesses pilares para realizar a leitura, organizaÃ§Ã£o, anÃ¡lise estatÃ­stica e apresentaÃ§Ã£o de informaÃ§Ãµes extraÃ­das de textos extensos, respeitando critÃ©rios de desempenho e uso eficiente de memÃ³ria.

A seguir, sÃ£o apresentados os principais conceitos teÃ³ricos que embasam a implementaÃ§Ã£o do sistema proposto.

---
### AnÃ¡lise lÃ©xica (Scanner)
Ã‰ o processo de decompor um texto ou cÃ³digo-fonte em unidades mÃ­nimas e significativas, denominadas *tokens* que sÃ£o usadas fundamentalmente em compiladores, interpretadores e em processamento de linguagem natural (PLN), para entender a estrutura e significado da entrada.

---
### AnÃ¡lise sintÃ¡tica (Parser)
Tem como funÃ§Ã£o receber os tokens do analisador lÃ©xico e verificar se eles formam uma estrutura gramaticalmente correta, de acordo com as regras da linguagem. TambÃ©m sÃ£o fundamentais para compiladores e PLN.

---
### Pilhas (Stacks)
A pilha Ã© uma estrutura de dados baseada no princÃ­pio FILO (First In, Last Out). Os elementos sÃ£o inseridos e removidos sempre pelo topo.
Neste projeto, pilhas sÃ£o usadas para:
* Verificar o balanceamento de sÃ­mbolos de pontuaÃ§Ã£o, como parÃªnteses, colchetes e chaves;
* Garantir a correta correspondÃªncia entre sÃ­mbolos de abertura e fechamento.

---
### Filas (Queues)
A fila segue o princÃ­pio FIFO (First In, First Out), em que o primeiro elemento inserido (enqueue) Ã© o primeiro a ser removido (dequeue).
As filas sÃ£o empregadas para:
* Armazenar sentenÃ§as e parÃ¡grafos na ordem em que aparecem no texto;
* Garantir o processamento sequencial correto das estruturas textuais.

---
### Listas Encadeadas (Linked Lists)
As listas encadeadas permitem inserÃ§Ã£o e remoÃ§Ã£o dinÃ¢mica de elementos, sem necessidade de realocaÃ§Ã£o contÃ­nua de memÃ³ria. Cada elemento aponta para o prÃ³ximo, formando uma sequÃªncia encadeada.
No contexto deste projeto, as listas encadeadas sÃ£o utilizadas para:
* Armazenar tokens e expressÃµes;
* Registrar ocorrÃªncias (linhas, posiÃ§Ãµes, sentenÃ§as);
* Servir como base para estruturas mais complexas.

---
### Tabelas Hash (Hash Tables)
Tabelas hash sÃ£o estruturas de dados que permitem acesso rÃ¡pido a elementos por meio de uma funÃ§Ã£o de dispersÃ£o (hash function). Elas oferecem, em mÃ©dia, complexidade de tempo constante para inserÃ§Ã£o, busca e remoÃ§Ã£o. A resoluÃ§Ã£o de colisÃµes Ã© feita por encadeamento, utilizando listas encadeadas nos buckets da tabela.
No projeto, tabelas hash sÃ£o utilizadas para:
* Armazenar tokens (palavras) e suas estatÃ­sticas;
* Registrar expressÃµes e suas ocorrÃªncias;
* Evitar duplicaÃ§Ã£o de entradas;
* Facilitar consultas rÃ¡pidas por texto.

---
### Mapas Associativos
Mapas associativos permitem relacionar chaves a valores. Neste trabalho, foi implementado um mapa especÃ­fico de inteiros (`IntIntMap`) sem o uso da biblioteca padrÃ£o STL.
Esse mapa Ã© utilizado para:
* Construir a distribuiÃ§Ã£o de comprimento das palavras;
* Associar o tamanho da palavra Ã  sua frequÃªncia de ocorrÃªncia.

---
### Algoritmos de OrdenaÃ§Ã£o
A ordenaÃ§Ã£o dos dados Ã© essencial para a apresentaÃ§Ã£o organizada dos resultados e para anÃ¡lises comparativas de desempenho.

---
#### Merge Sort
O Merge Sort Ã© um algoritmo de ordenaÃ§Ã£o baseado na estratÃ©gia "dividir para conquistar". Ele divide o vetor em partes menores, ordena cada parte e, em seguida, realiza a fusÃ£o ordenada.
CaracterÃ­sticas:
* Complexidade do pior e melhor caso: $$O(n\;log\;n)$$
* EstÃ¡vel
* Custo adicional de memÃ³ria para copiar o vetor.
* NÃ£o hÃ¡ melhora se os elementos jÃ¡ estiverem parcialmente ordenados.
* NÃ£o faz troca de elementos, pois eles se ordenam quando as partes se fundem (merge).

---
#### Quick Sort
O Quick Sort tambÃ©m segue a estratÃ©gia de "dividir para conquistar", escolhendo um pivÃ´ e particionando o vetor em elementos menores e maiores que ele.
CaracterÃ­sticas:
* Complexidade mÃ©dia: $$O(n\;log\;n)$$
* Pior caso: $$O(n^2)$$
* InstÃ¡vel, pode alterar a ordem dos elementos de mesmo valor.
* Custo de memÃ³ria extra relacionado a recursÃ£o na pilha.

---
#### Merge Sort X Quick Sort
|CaracterÃ­stica|**MergeSort**| **QuickSort**|
|-|-|-|
|**EstratÃ©gia**|Dividir  para conquistar|Dividir  para conquistar|
|**Complexidade (Melhor Caso)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|
|**Complexidade (Caso MÃ©dio)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|
|**Complexidade (Pior Caso)**|$$O(n\;log\;n)$$|$$O(n^2)$$|
|**Estabilidade**|EstÃ¡vel|InstÃ¡vel|
|**Adapatabilidade**|NÃ£o|Sim|
|**MemÃ³ria extra**|$$O(n)$$|$$O(log\;n)$$|
|**MovimentaÃ§Ãµes (Swaps)**|$$O(n\;log\;n)$$|$$O(n\;log\;n)$$|
> **Tabela 1 - Comparativo das caracteriscas do Merge Sort e do Quick Sort.**

---
## ğŸ”¬ Modelagem da AplicaÃ§Ã£o
A partir do objetivo de desenvolver um sistema analisador lÃ©xico-sintÃ¡tico, a modelagem teve como princÃ­pio equilibrar o desempenho computacional com a construÃ§Ã£o de um sistema robusto e modular. O projeto foi implementado em C++, visando eficiÃªncia computacional, controle explÃ­cito de memÃ³ria e flexibilidade na implementaÃ§Ã£o de estruturas personalizadas com POO.

A arquitetura do sistema segue uma abordagem modular, na qual cada componente possui responsabilidades bem definidas, facilitando tanto a manutenÃ§Ã£o quanto a anÃ¡lise individual de desempenho. O fluxo geral da aplicaÃ§Ã£o Ã© coordenado pela funÃ§Ã£o `main`, que orquestra a leitura do texto, a anÃ¡lise lÃ©xica, o armazenamento dos dados e a geraÃ§Ã£o dos relatÃ³rios e mÃ©tricas experimentais.

---
### Fluxo de ExecuÃ§Ã£o do Sistema
O funcionamento do analisador pode ser dividido em etapas sequenciais, que refletem o *pipeline* de processamento textual adotado no projeto:
1. **Leitura do Texto de Entrada**
   O sistema inicia com a leitura do arquivo de texto fornecido via linha de comando. Essa etapa Ã© realizada pela classe `TextReader`, responsÃ¡vel por:

   * Abrir e validar o arquivo de entrada;
   * Ler o texto linha a linha;
   * Manter o controle do nÃºmero da linha atual.
2. **AnÃ¡lise LÃ©xica**
   A etapa central do sistema Ã© conduzida pela classe `Analyzer`, que processa cada linha do texto e executa as seguintes operaÃ§Ãµes:

   * NormalizaÃ§Ã£o das palavras (remoÃ§Ã£o de acentos, conversÃ£o para minÃºsculas, tratamento de caracteres especiais);
   * IdentificaÃ§Ã£o e filtragem de *stopwords*, definidas em `stopwords.txt`;
   * TokenizaÃ§Ã£o do texto em palavras (*tokens*);
   * Registro das ocorrÃªncias de cada token (parÃ¡grafo, sentenÃ§a, linha e posiÃ§Ã£o);
   * DetecÃ§Ã£o de sentenÃ§as e parÃ¡grafos;
   * VerificaÃ§Ã£o do balanceamento de sÃ­mbolos de pontuaÃ§Ã£o por meio de pilhas;
   * IdentificaÃ§Ã£o e contagem de expressÃµes previamente definidas em `expressoes.txt`.
3. **Armazenamento em Estruturas de Dados**
   Ainda na class ``Analyzer``, os dados extraÃ­dos durante a anÃ¡lise sÃ£o armazenados utilizando estruturas implementadas manualmente no projeto, tais como:

   * Tabelas hash para tokens e expressÃµes;
   * Listas encadeadas para armazenar coleÃ§Ãµes dinÃ¢micas;
   * Filas para preservar a ordem de sentenÃ§as e parÃ¡grafos;
   * Pilhas para anÃ¡lise de balanceamento de sÃ­mbolos.
4. **GeraÃ§Ã£o de RelatÃ³rios e ExportaÃ§Ã£o de Dados**
   A classe `Report` Ã© responsÃ¡vel por:

   * Gerar relatÃ³rios textuais detalhados com os resultados da anÃ¡lise;
   * Exportar mÃ©tricas de desempenho e distribuiÃ§Ãµes estatÃ­sticas em arquivos `.csv`;
   * Produzir dados que posteriormente sÃ£o utilizados para a geraÃ§Ã£o de grÃ¡ficos.
5. **VisualizaÃ§Ã£o GrÃ¡fica (PÃ³s-processamento)**
   Scripts auxiliares em Python utilizam os arquivos `.csv` gerados para:

   * Plotar grÃ¡ficos de distribuiÃ§Ã£o do comprimento das palavras;
   * Comparar o desempenho dos algoritmos de ordenaÃ§Ã£o em funÃ§Ã£o do tamanho da entrada.

Essa separaÃ§Ã£o permite o desenvolvimento e teste modular da aplicaÃ§Ã£o, alÃ©m de facilitar a manutenÃ§Ã£o e refatoraÃ§Ã£o do cÃ³digo, caso necessÃ¡rio.

**Fluxograma de execuÃ§Ã£o do projeto.**

---
### ğŸ“Š Estruturas de Dados

O projeto faz uso extensivo de estruturas de dados implementadas manualmente, evitando o uso direto de containers prontos da STL para fins educacionais. Entre as principais estruturas utilizadas, destacam-se:

* **Lista Encadeada (`LinkedList`)**
  Utilizada como base para diversas outras estruturas, permitindo armazenamento dinÃ¢mico de elementos. Utiliza o recurso de templates do C++, tornando-a uma estrutura genÃ©rica, podendo listar qualquer tipo de elemento. Possui complexidade de inserÃ§Ã£o, remoÃ§Ã£o e acesso de O(n).

* **Tabela Hash (`HashTable`)**
  Empregada para armazenar tokens e expressÃµes, permitindo acesso eficiente por chave textual. TambÃ©m utiliza de template, porÃ©m Ã© generalizada somente para elementos que possuem o mÃ©todo `addOccurrence`. Possui complexidade de inserÃ§Ã£o e acesso O(1) no melhor caso e O(n) no pior caso, sendo n o tamanho do *bucket*.

* **Fila (`Queue`)**
  Utilizada para manter a ordem de sentenÃ§as e parÃ¡grafos durante o processamento e a geraÃ§Ã£o de relatÃ³rios. Possui complexidade de inserÃ§Ã£o, acesso e remoÃ§Ã£o O(1), graÃ§as aos ponteiros de `front` e `rear`.

* **Pilha (`Stack`)**
  Aplicada na verificaÃ§Ã£o do balanceamento de sÃ­mbolos de pontuaÃ§Ã£o, como parÃªnteses e colchetes. Possui complexidade de inserÃ§Ã£o, acesso e remoÃ§Ã£o O(1).

* **Mapeamento Inteiroâ€“Inteiro (`IntIntMap`)**
  Utilizado para armazenar distribuiÃ§Ãµes estatÃ­sticas, como a frequÃªncia de palavras por comprimento. Possui inserÃ§Ã£o e acesso O(1).

Essas estruturas foram projetadas visando clareza conceitual, eficiÃªncia e integraÃ§Ã£o com os algoritmos de ordenaÃ§Ã£o e anÃ¡lise estatÃ­stica.

---
### ğŸš€ OtimizaÃ§Ãµes Propostas

#### PrÃ©-processamento Textual

* NormalizaÃ§Ã£o antecipada das palavras para reduzir comparaÃ§Ãµes redundantes;
* RemoÃ§Ã£o de *stopwords* ainda na fase de anÃ¡lise, reduzindo o volume de dados armazenados.

#### Uso de Hashing

* UtilizaÃ§Ã£o de funÃ§Ã£o hash simples e eficiente para distribuiÃ§Ã£o uniforme dos tokens;
* ReduÃ§Ã£o do custo mÃ©dio de inserÃ§Ã£o e busca na tabela de sÃ­mbolos.

#### AnÃ¡lise EmpÃ­rica de OrdenaÃ§Ã£o

* ExecuÃ§Ã£o de mÃºltiplos testes com diferentes tamanhos de entrada;
* ComparaÃ§Ã£o entre MergeSort e QuickSort sob diferentes critÃ©rios de ordenaÃ§Ã£o;
* Coleta detalhada de mÃ©tricas para validaÃ§Ã£o empÃ­rica das complexidades assintÃ³ticas esperadas.

---
## ğŸ“ Metodologia
A implementaÃ§Ã£o do sistema de anÃ¡lise lÃ©xica e sintÃ¡tica foram feitas em C++, uitilizando a IDE Visual Studio Code para desenvolvimento do cÃ³digo-fonte e uso do Github para controle de versÃ£o. O projeto foi organizado em um repositÃ³rio, contendo diretÃ³rios dividindo os arquivos de cabeÃ§alho (.hpp) em ``include``, arquivos de implementaÃ§Ã£o (.cpp) em ``src``, arquivos de entrada em ``data``, scripts em Python no diretÃ³rio ``utils``, diretÃ³rio de saÃ­da ``output``, alÃ©m de outros diretÃ³rios auxiliares.

---
## ğŸ“ Estrutura do Projeto
A seguir, a estrutura do diretÃ³rio do projeto, organizada para separar o cÃ³digo-fonte, os dados e os resultados:
```
.
â”‚
â”œâ”€â”€ bin/
|   â””â”€â”€ LSA
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ DomCasmurro.txt
â”‚   â”œâ”€â”€ expressoes.txt
â”‚   â”œâ”€â”€ output.txt
|   â”œâ”€â”€ Semana_Machado_Assis.txt
â”‚   â””â”€â”€ stopwords.txt
â”‚
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ Analyzer.hpp
â”‚   â”œâ”€â”€ Expression.hpp
â”‚   â”œâ”€â”€ ExpressionComparators.hpp
â”‚   â”œâ”€â”€ HashTable.hpp
â”‚   â”œâ”€â”€ IntIntMap.hpp
â”‚   â”œâ”€â”€ LinkedList.hpp
â”‚   â”œâ”€â”€ MapEntry.hpp
â”‚   â”œâ”€â”€ MapEntryComparators.hpp
â”‚   â”œâ”€â”€ Node.hpp
â”‚   â”œâ”€â”€ Occurrence.hpp
â”‚   â”œâ”€â”€ Paragraph.hpp
â”‚   â”œâ”€â”€ Queue.hpp
â”‚   â”œâ”€â”€ Report.hpp
â”‚   â”œâ”€â”€ Sentence.hpp
â”‚   â”œâ”€â”€ Sorter.hpp
â”‚   â”œâ”€â”€ SortMetrics.hpp
â”‚   â”œâ”€â”€ Stack.hpp
â”‚   â”œâ”€â”€ TextReader.hpp
â”‚   â”œâ”€â”€ Token.hpp
|   â””â”€â”€ TokenComparators.hpp
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ length_dist.csv
â”‚   â”œâ”€â”€ length_distribution.png
â”‚   â”œâ”€â”€ output.txt
â”‚   â”œâ”€â”€ sort_metrics.csv
â”‚   â”œâ”€â”€ sort_performance_comparisons.png
â”‚   â”œâ”€â”€ sort_performance_swaps.png
â”‚   â””â”€â”€ sort_performance_time.png
|
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Analyzer.cpp
â”‚   â”œâ”€â”€ Expression.cpp
â”‚   â”œâ”€â”€ IntIntMap.cpp
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ Paragraph.cpp
â”‚   â”œâ”€â”€ Report.cpp
â”‚   â”œâ”€â”€ Sentence.cpp
â”‚   â”œâ”€â”€ TextReader.cpp
â”‚   â””â”€â”€ Token.cpp
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ plot_length.py
â”‚   â”œâ”€â”€ plot_sort_metrics.py
â”‚   â””â”€â”€ plot_utils.py
|
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Doxyfile
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

---
### ğŸ“ Arquivos e DiretÃ³rios
A seguir, sÃ£o descritos os principais arquivos e diretÃ³rios do projeto, detalhando suas responsabilidades no processo de anÃ¡lise lÃ©xica e sintÃ¡tica.
* **`bin/`**
  ContÃ©m o executÃ¡vel final do projeto apÃ³s a compilaÃ§Ã£o.
  * `LSA`: executÃ¡vel do *Lexical-Syntactic Analyzer*.
* **`data/`**
  DiretÃ³rio que armazena os arquivos de entrada utilizados na anÃ¡lise textual:
  * `DomCasmurro.txt`: texto literÃ¡rio utilizado como base principal para anÃ¡lise.
  * `expressoes.txt`: arquivo contendo expressÃµes especÃ­ficas a serem identificadas no texto.
  * `output.txt`: arquivo de exemplo utilizado para montar o ``output.txt`` do projeto.
  * `Semana_Machado_Assis.txt`: texto alternativo para testes comparativos.
  * `stopwords.txt`: lista de palavras irrelevantes (artigos, preposiÃ§Ãµes etc.) removidas durante a anÃ¡lise lÃ©xica.
* **`include/`**
  ContÃ©m todos os arquivos de cabeÃ§alho (`.hpp`), responsÃ¡veis pela definiÃ§Ã£o das estruturas de dados, classes, classes template e interfaces do sistema.
* **`src/`**
  ContÃ©m os arquivos de implementaÃ§Ã£o (`.cpp`) correspondentes aos cabeÃ§alhos definidos em `include/`.
* **`output/`**
  DiretÃ³rio de saÃ­da que armazena os resultados da execuÃ§Ã£o do analisador, incluindo arquivos de texto, mÃ©tricas e grÃ¡ficos gerados:
  * `output.txt`: relatÃ³rio textual com os resultados da anÃ¡lise lÃ©xica e sintÃ¡tica.
  * `length_dist.csv`: distribuiÃ§Ã£o de tokens por comprimento.
  * `sort_metrics.csv`: mÃ©tricas coletadas durante os algoritmos de ordenaÃ§Ã£o.
  * Arquivos `.png`: grÃ¡ficos de desempenho e anÃ¡lise estatÃ­stica.
* **`utils/`**
  Scripts auxiliares em Python utilizados para plotagem de grÃ¡ficos e anÃ¡lise visual dos dados:
  * `plot_length.py`: gera grÃ¡fico da distribuiÃ§Ã£o de comprimentos dos tokens.
  * `plot_sort_metrics.py`: gera grÃ¡ficos comparativos de desempenho dos algoritmos de ordenaÃ§Ã£o.
  * `plot_utils.py`: funÃ§Ãµes auxiliares comuns aos scripts de plotagem.

---
### ğŸ“š Bibliotecas
A implementaÃ§Ã£o do Analisador LÃ©xico e SintÃ¡tico nÃ£o utiliza das estruturas de dados do STL, porÃ©m utiliza de outras bibliotecas padrÃµes do C++. As principais bibliotecas sÃ£o:

---
### ğŸ“Œ Bibliotecas do Projeto (Headers Customizados)
Os principais arquivos de cabeÃ§alho desenvolvidos no projeto sÃ£o:
* **`Analyzer.hpp`**
  Define a classe central do sistema, responsÃ¡vel por orquestrar a anÃ¡lise lÃ©xica e sintÃ¡tica, integrando a leitura do texto, tokenizaÃ§Ã£o, filtragem de *stopwords*, identificaÃ§Ã£o de expressÃµes e geraÃ§Ã£o dos relatÃ³rios finais.
* **`TextReader.hpp`**
  ResponsÃ¡vel pela leitura dos arquivos de entrada e controle da linha atual e o nÃºmero da linha.
* **`Token.hpp`**
  Define a estrutura de um token lÃ©xico, contendo informaÃ§Ãµes como o texto, e a lista de ocorrencias do token.
* **`Sentence.hpp`** e **`Paragraph.hpp`**
  Representam a organizaÃ§Ã£o hierÃ¡rquica do texto, permitindo armazenar informaÃ§Ãµes de sentenÃ§as e parÃ¡grafos para uso posterior em fila.

* **`Expression.hpp`**
  Representa expressÃµes compostas detectadas no texto, com uma lista de linhas onde foram encontradas determinada expressÃ£o..

* **`Occurrence.hpp`**
  Armazena informaÃ§Ãµes sobre as ocorrÃªncias de tokens, como sentenÃ§a, parÃ¡grafo, linha e posiÃ§Ã£o.

* **`HashTable.hpp`**
  ImplementaÃ§Ãµes customizadas de tabelas hash genÃ©rico apenas para objetos com mÃ©todo `addOccurrence`.

* *`IntIntMap.hpp`**
  ImplementaÃ§Ã£o de mapa de inteiro para inteiro, utilizado para calcular distribuiÃ§Ã£o por comprimento dos tokens do texto de entrada.

* *`Node.hpp`**
  ImplementaÃ§Ã£o de nÃ³ simples genÃ©rico, contendo o dado template `T` e o ponteiro para prÃ³ximo nÃ³.

* **`LinkedList.hpp`**
  Implementa listas encadeadas genÃ©ricas utilizadas para armazenar coleÃ§Ãµes dinÃ¢micas de tokens, expressÃµes e ocorrÃªncias.

* **`Stack.hpp`** e **`Queue.hpp`**
  Estruturas auxiliares empregadas em etapas especÃ­ficas da anÃ¡lise sintÃ¡tica e no controle de fluxo de dados.

* **`Sorter.hpp`**
  Implementa os dois algoritmos de ordenaÃ§Ã£o selecionados, aplicados aos tokens, expressÃµes e entradas do mapa, permitindo a anÃ¡lise de desempenho conforme critÃ©rios alfabÃ©ticos e por frequÃªncia.

* **`SortMetrics.hpp`**
  Estrutura responsÃ¡vel por coletar mÃ©tricas de desempenho dos algoritmos de ordenaÃ§Ã£o, como nÃºmero de comparaÃ§Ãµes, trocas e tempo de execuÃ§Ã£o.

* **`Report.hpp`**
  ResponsÃ¡vel pela consolidaÃ§Ã£o dos resultados da anÃ¡lise e pela geraÃ§Ã£o dos arquivos de saÃ­da em formato textual.

* **Arquivos de Comparadores (`ExpressionComparators.hpp`, `TokenComparators.hpp`, `MapEntryComparators.hpp`)**
  Definem critÃ©rios personalizados de comparaÃ§Ã£o utilizados nos processos de ordenaÃ§Ã£o e ranqueamento.

---
### ğŸ“Œ Bibliotecas PadrÃ£o do C++

A seguir estÃ£o listadas as principais bibliotecas padrÃ£o do C++ utilizadas no projeto:

* **`iostream`**
  Utilizada para entrada e saÃ­da de dados no console, especialmente para mensagens de execuÃ§Ã£o e depuraÃ§Ã£o.

* **`fstream`**
  Empregada na leitura dos arquivos de entrada (`.txt`) e na escrita dos arquivos de saÃ­da gerados pelo analisador.

* **`string`**
  Essencial para a manipulaÃ§Ã£o de cadeias de caracteres durante a tokenizaÃ§Ã£o e anÃ¡lise lÃ©xica.

* **`chrono`**
  Utilizado para mediÃ§Ã£o precisa do tempo de execuÃ§Ã£o dos algoritmos de ordenaÃ§Ã£o, permitindo a anÃ¡lise de desempenho.

* **`cctype`**
  Utilizado para classificaÃ§Ã£o de caracteres durante a anÃ¡lise lÃ©xica (letras, dÃ­gitos, pontuaÃ§Ã£o, etc.).

* **`iomanip`**
  Utilizado para formataÃ§Ã£o da saÃ­da de dados numÃ©ricos nos relatÃ³rios.

* **`cstdlib`** e **`cstring`**
  Utilizados para operaÃ§Ãµes bÃ¡sicas de conversÃ£o e manipulaÃ§Ã£o de dados em baixo nÃ­vel quando necessÃ¡rio.

---
## âš™ï¸ Estruturas de Dados e ImplementaÃ§Ã£o do Sistema
Nesta seÃ§Ã£o sÃ£o descritas e explicadas as classes e estruturas de dados desenvolvidas para o projeto, evidenciando seus atributos e mÃ©todos.

---
### ğŸ§± Estruturas Base do Sistema

---
#### `struct Occurrence`
Representa uma ocorrÃªncia individual de um token no texto.

**Atributos:**
* `paragraph`: NÃºmero do parÃ¡grafo onde ocorre.
* `sentence`: NÃºmero da sentenÃ§a onde ocorre.
* `line`: NÃºmero da linha onde ocorre.
* `position`: PosiÃ§Ã£o do token dentro da sentenÃ§a.

**MÃ©todos:**
* `Occurrence(int p, int s, int l, int pos)`: Construtor que inicializa todos os campos posicionais da ocorrÃªncia.

---
#### `class Token`
Representa um token lÃ©xico (palavra) extraÃ­do do texto.

**Atributos:**
* `text`: Texto do token.
* `frequency`: NÃºmero total de ocorrÃªncias do token no texto.
* `occurrences`: Lista encadeada contendo todas as ocorrÃªncias do token.

**MÃ©todos:**
* `Token()`: Construtor padrÃ£o.
* `Token(const string &t)`: Construtor que inicializa o token com um texto especÃ­fico.
* `Token(const Token &other)`: Construtor de cÃ³pia.
* `Token& operator=(const Token &other)`: Operador de atribuiÃ§Ã£o para cÃ³pia de tokens.
* `addOccurrence(int p, int s, int l, int pos)`: Registra uma nova ocorrÃªncia do token, atualizando a frequÃªncia.
* `getText()`: Retorna o texto do token.
* `getFrequency()`: Retorna a frequÃªncia total do token.
* `getOccurrences()`: Retorna a lista de ocorrÃªncias associadas ao token.

---
#### `class Sentence`
Armazena informaÃ§Ãµes estatÃ­sticas associadas a uma sentenÃ§a do texto.

**Atributos:**
* `paragraphNumber`: NÃºmero do parÃ¡grafo ao qual a sentenÃ§a pertence.
* `sentenceNumber`: Ãndice da sentenÃ§a dentro do parÃ¡grafo.
* `stopWords`: Quantidade de *stopwords* na sentenÃ§a.
* `nonStopWords`: Quantidade de palavras relevantes na sentenÃ§a.
* `averageWordLength`: Comprimento mÃ©dio das palavras da sentenÃ§a.

**MÃ©todos:**
* `Sentence()`: Construtor padrÃ£o.
* `Sentence(int pN, int sN, int sWords, int nWords, double avrWordLength)`: Construtor que inicializa todas as mÃ©tricas da sentenÃ§a.
* `getParagraphNumber()`: Retorna o nÃºmero do parÃ¡grafo.
* `getSentenceNumber()`: Retorna o nÃºmero da sentenÃ§a.
* `getAllWords()`: Retorna o total de palavras da sentenÃ§a (com e sem *stopwords*).
* `getNormalWords()`: Retorna o nÃºmero de palavras nÃ£o consideradas *stopwords*.
* `getAverageWordLength()`: Retorna o comprimento mÃ©dio das palavras.

---
#### `class Paragraph`
Representa um parÃ¡grafo do texto analisado.

**Atributos:**
* `number`: NÃºmero sequencial do parÃ¡grafo no texto.
* `startingLine`: NÃºmero da linha onde o parÃ¡grafo se inicia.
* `totalSentences`: Quantidade de sentenÃ§as no parÃ¡grafo.

**MÃ©todos:**
* `Paragraph()`: Construtor padrÃ£o.
* `Paragraph(int n, int sL, int tS)`: Construtor que inicializa os metadados do parÃ¡grafo.
* `getNumber()`: Retorna o nÃºmero do parÃ¡grafo.
* `getStartingLine()`: Retorna a linha inicial do parÃ¡grafo.
* `getTotalSentences()`: Retorna o nÃºmero total de sentenÃ§as.

---
#### `class Expression`
Representa uma expressÃ£o composta por mÃºltiplas palavras.

**Atributos:**
* `text`: Texto completo da expressÃ£o.
* `frequency`: NÃºmero de ocorrÃªncias da expressÃ£o no texto.
* `lines`: Lista encadeada contendo as linhas onde a expressÃ£o ocorre.

**MÃ©todos:**
* `Expression()`: Construtor padrÃ£o.
* `Expression(const string &t)`: Construtor que inicializa a expressÃ£o com um texto especÃ­fico.
* `Expression(const Expression &other)`: Construtor de cÃ³pia.
* `Expression& operator=(const Expression &other)`: Operador de atribuiÃ§Ã£o.
* `addOccurrence(int line)`: Registra uma nova ocorrÃªncia da expressÃ£o.
* `getText()`: Retorna o texto da expressÃ£o.
* `getFrequency()`: Retorna a frequÃªncia da expressÃ£o.
* `getLines()`: Retorna a lista de linhas onde a expressÃ£o ocorre.

---
### ğŸ§± Estruturas de Dados GenÃ©ricas

---
#### `struct Node<T>`
Estrutura fundamental para a implementaÃ§Ã£o de listas encadeadas simples, servindo como base para estruturas lineares como pilhas, filas e listas.
A implementaÃ§Ã£o explÃ­cita da estrutura `Node` permite compreender e controlar diretamente o gerenciamento dinÃ¢mico de memÃ³ria, alÃ©m de servir como elemento unificador para diferentes estruturas de dados lineares, evitando duplicaÃ§Ã£o de cÃ³digo e reforÃ§ando o conceito de abstraÃ§Ã£o estrutural.

**Atributos:**
* `data`: Elemento armazenado no nÃ³.
* `next`: Ponteiro para o prÃ³ximo nÃ³ da estrutura.

**MÃ©todos:**
* `Node(const T &value)`: Construtor que inicializa o nÃ³ com um valor e ponteiro nulo para o prÃ³ximo elemento.

---
#### `class Stack<T>`
ImplementaÃ§Ã£o genÃ©rica de uma pilha seguindo a polÃ­tica FILO.

**Atributos:**
* `topNode`: Ponteiro para o elemento do topo da pilha.
* `size`: NÃºmero total de elementos armazenados.

**MÃ©todos:**
* `Stack()`: Construtor padrÃ£o.
* `Stack(const Stack<T> &other)`: Construtor de cÃ³pia preservando a ordem dos elementos.
* `~Stack()`: Destrutor que libera toda a memÃ³ria alocada.
* `operator=`: Operador de atribuiÃ§Ã£o.
* `push(const T &value)`: Insere um elemento no topo da pilha (*O(1)*).
* `pop(T &removed)`: Remove o elemento do topo (*O(1)*).
* `peek(T &value)`: Consulta o elemento do topo sem removÃª-lo (*O(1)*).
* `isEmpty()`: Verifica se a pilha estÃ¡ vazia.
* `getSize()`: Retorna o nÃºmero de elementos.
* `clear()`: Remove todos os elementos da pilha.

---
#### `class Queue<T>`
ImplementaÃ§Ã£o genÃ©rica de uma fila seguindo a polÃ­tica FIFO.

**Atributos:**
* `front`: Ponteiro para o primeiro elemento da fila.
* `rear`: Ponteiro para o Ãºltimo elemento da fila.
* `size`: NÃºmero total de elementos na fila.

**MÃ©todos:**
* `Queue()`: Construtor padrÃ£o.
* `Queue(const Queue<T> &other)`: Construtor de cÃ³pia.
* `~Queue()`: Destrutor que libera a memÃ³ria.
* `operator=`: Operador de atribuiÃ§Ã£o.
* `enqueue(const T &value)`: Insere um elemento no final da fila (*O(1)*).
* `dequeue(T &removed)`: Remove o elemento do inÃ­cio da fila (*O(1)*).
* `isEmpty()`: Verifica se a fila estÃ¡ vazia.
* `getSize()`: Retorna o nÃºmero de elementos.
* `clear()`: Remove todos os elementos da fila.

---
#### `class LinkedList<T>`
ImplementaÃ§Ã£o genÃ©rica de uma lista encadeada simples.

**Atributos:**
* `head`: Ponteiro para o primeiro nÃ³ da lista.
* `size`: NÃºmero de elementos armazenados.

**MÃ©todos:**
* `LinkedList()`: Construtor padrÃ£o.
* `LinkedList(const LinkedList<T> &other)`: Construtor de cÃ³pia.
* `~LinkedList()`: Destrutor.
* `operator=`: Operador de atribuiÃ§Ã£o.
* `insert(const T &value)`: Insere um elemento no final da lista (*O(n)*).
* `get(int i)`: Retorna o elemento na posiÃ§Ã£o `i`.
* `isEmpty()`: Verifica se a lista estÃ¡ vazia.
* `getSize()`: Retorna o tamanho da lista.
* `clear()`: Remove todos os elementos da lista.
* `toArray(int &outSize)`: Converte a lista em um vetor dinÃ¢mico.
* `Iterator`: Classe interna que permite iteraÃ§Ã£o sequencial sobre os elementos da lista.

---
#### `class HashTable<T>`
ImplementaÃ§Ã£o de tabela hash com "encadeamento separado" para tratamento de colisÃµes.
Nesta implementaÃ§Ã£o, a estrutura nÃ£o Ã© totalmente genÃ©rica, pois assume que o tipo armazenado possui mÃ©todos como `getText()` e construtores especÃ­ficos, refletindo uma adaptaÃ§Ã£o prÃ¡tica ao domÃ­nio do problema.

**Atributos:**
* `TABLE_SIZE`: Tamanho fixo da tabela.
* `table`: Vetor de listas encadeadas usado como buckets.

**MÃ©todos:**
* `HashTable()`: Construtor padrÃ£o.
* `HashTable(const HashTable &other)`: Construtor de cÃ³pia.
* `~HashTable()`: Destrutor.
* `insert(const string &key, int ...)`: Insere tokens com suas ocorrÃªncias.
* `insert(const string &key, int line)`: Insere expressÃµes e suas ocorrÃªncias.
* `getBucket(int index)`: Retorna um bucket especÃ­fico.
* `getTableSize()`: Retorna o tamanho da tabela.
* `countObjects()`: Conta o total de elementos armazenados.
* `contains(const string &key)`: Verifica se uma chave existe.
* `toArray(int &outSize)`: Converte a tabela hash em um vetor dinÃ¢mico.
* `clear()`: Remove todos os elementos da tabela.

---
### ğŸ—ºï¸ Estruturas Auxiliares de Mapeamento

---
#### `struct MapEntry`
Estrutura que representa um par chaveâ€“valor inteiro, utilizada como elemento bÃ¡sico em mapas personalizados.
A separaÃ§Ã£o explÃ­cita da estrutura `MapEntry` permite desacoplar o conceito de *entrada de mapa* da implementaÃ§Ã£o da estrutura de dados que a armazena. Isso facilita a reutilizaÃ§Ã£o em diferentes contextos, alÃ©m de tornar o cÃ³digo mais modular e semanticamente claro em aplicaÃ§Ãµes de contagem e distribuiÃ§Ã£o de frequÃªncias.

**Atributos:**
* `key`: Chave inteira associada Ã  entrada do mapa.
* `value`: Valor inteiro associado Ã  chave, normalmente utilizado como contador de frequÃªncia.

**MÃ©todos:**
* `MapEntry(int key = 0, int value = 0)`: Construtor que inicializa a chave e o valor da entrada.

---
#### `class IntIntMap`

ImplementaÃ§Ã£o personalizada de um mapa (inteiro â†’ inteiro), baseada em lista encadeada.
A criaÃ§Ã£o do `IntIntMap` se justifica pela necessidade de evitar o uso da STL e, ao mesmo tempo, oferecer uma estrutura simples e controlada para contagem de frequÃªncias. Embora apresente complexidade linear, essa abordagem Ã© adequada para conjuntos de dados moderados e reforÃ§a o entendimento dos mecanismos internos de estruturas associativas.
AlÃ©m disso, essa implementaÃ§Ã£o resolve limitaÃ§Ãµes da `HashTable<T>` genÃ©rica, que nÃ£o suporta naturalmente pares chaveâ€“valor, sendo necessÃ¡ria uma estrutura especializada para mapear inteiros de forma direta e sem dependÃªncias externas.

**Atributos:**
* `list`: Lista encadeada de `MapEntry`, responsÃ¡vel por armazenar os pares chaveâ€“valor.

**MÃ©todos:**
* `IntIntMap()`: Construtor padrÃ£o.
* `~IntIntMap()`: Destrutor.
* `increment(int key)`: Incrementa o valor associado Ã  chave em uma unidade (*O(n)*).
* `increment(int key, int amount)`: Incrementa o valor associado Ã  chave por um valor arbitrÃ¡rio (*O(n)*).
* `get(int key)`: Retorna o valor associado Ã  chave, ou `0` caso nÃ£o exista (*O(n)*).
* `size()`: Retorna o nÃºmero de entradas armazenadas (*O(1)*).
* `getEntries()`: Retorna uma referÃªncia para a lista de entradas.
* `clear()`: Remove todas as entradas do mapa.

---
### ğŸ”€ Estruturas de ComparaÃ§Ã£o e OrdenaÃ§Ã£o
A separaÃ§Ã£o das funÃ§Ãµes de comparaÃ§Ã£o em um mÃ³dulo especÃ­fico promove desacoplamento entre os algoritmos de ordenaÃ§Ã£o e os critÃ©rios de ordenaÃ§Ã£o. Isso permite reutilizar os mesmos algoritmos (Merge Sort e Quick Sort) com diferentes polÃ­ticas de ordenaÃ§Ã£o, alÃ©m de facilitar a coleta de mÃ©tricas experimentais.

---
#### FunÃ§Ãµes de ComparaÃ§Ã£o para `Expression` (`ExpressionComparators.hpp`)
##### `expressionAlpha`
* **FunÃ§Ã£o:** Comparador alfabÃ©tico para objetos `Expression`.
* **CritÃ©rio:** Ordem lexicogrÃ¡fica crescente do texto da expressÃ£o.
* **Aspecto analÃ­tico:** Incrementa o contador de comparaÃ§Ãµes em `SortMetrics`.

##### `expressionFreq`
* **FunÃ§Ã£o:** Comparador por frequÃªncia de ocorrÃªncia.
* **CritÃ©rio:** Ordem decrescente de frequÃªncia.
* **Aspecto analÃ­tico:** Incrementa o contador de comparaÃ§Ãµes para anÃ¡lise de desempenho.

---
#### FunÃ§Ãµes de ComparaÃ§Ã£o para `Token` (`TokenComparators.hpp`)
##### `tokenAlpha`
* **FunÃ§Ã£o:** Comparador alfabÃ©tico entre tokens.
* **CritÃ©rio:** Ordem lexicogrÃ¡fica crescente do texto do token.
* **Aspecto analÃ­tico:** Atualiza o nÃºmero de comparaÃ§Ãµes em `SortMetrics`.

##### `tokenFreq`
* **FunÃ§Ã£o:** Comparador por frequÃªncia de ocorrÃªncia.
* **CritÃ©rio:** Ordem decrescente da frequÃªncia do token.
* **Aspecto analÃ­tico:** Utilizado para anÃ¡lises de palavras mais recorrentes.

---
#### FunÃ§Ãµes de ComparaÃ§Ã£o para `MapEntry` (`MapEntryComparators.hpp`)
##### `mapEntryKeyAsc`
* **FunÃ§Ã£o:** Comparador por chave.
* **CritÃ©rio:** Ordem crescente da chave inteira.
* **Uso tÃ­pico:** AnÃ¡lise ordenada de categorias ou intervalos.

##### `mapEntryValueDesc`
* **FunÃ§Ã£o:** Comparador por valor.
* **CritÃ©rio:** Ordem decrescente do valor.
* **Uso tÃ­pico:** IdentificaÃ§Ã£o de frequÃªncias dominantes.

---
#### `struct SortMetrics`
Estrutura responsÃ¡vel por armazenar mÃ©tricas de desempenho de algoritmos de ordenaÃ§Ã£o.
A instrumentaÃ§Ã£o explÃ­cita dos algoritmos permite a avaliaÃ§Ã£o empÃ­rica de desempenho, essencial em disciplinas de AnÃ¡lise de Algoritmos e Estruturas de Dados. Essa abordagem possibilita comparar algoritmos sob diferentes entradas e critÃ©rios de ordenaÃ§Ã£o.

**Atributos:**
* `comparisons`: NÃºmero total de comparaÃ§Ãµes realizadas.
* `swaps`: NÃºmero de trocas efetuadas durante a ordenaÃ§Ã£o.
* `elapsedTime`: Tempo total de execuÃ§Ã£o do algoritmo (em segundos).

**MÃ©todos:**
* `SortMetrics()`: Inicializa todas as mÃ©tricas com zero.
* `clear()`: Reseta todas as mÃ©tricas para nova execuÃ§Ã£o experimental.

---
#### `class Sorter<T>`
Classe utilitÃ¡ria genÃ©rica que implementa algoritmos clÃ¡ssicos de ordenaÃ§Ã£o.
A implementaÃ§Ã£o manual de algoritmos como Merge Sort e Quick Sort reforÃ§a o entendimento de suas propriedades teÃ³ricas e prÃ¡ticas. O uso de templates permite aplicar os mesmos algoritmos a diferentes tipos de dados, enquanto os comparadores externos transformam os mÃ©todos da classe em funÃ§Ãµes de ordem superior, possibilitam flexibilidade sem duplicaÃ§Ã£o de cÃ³digo.

**Principais caracterÃ­sticas:**
* ImplementaÃ§Ã£o genÃ©rica baseada em templates.
* Coleta automÃ¡tica de mÃ©tricas de desempenho.
* Suporte a diferentes critÃ©rios de ordenaÃ§Ã£o via funÃ§Ã£o comparadora.

**Tipos Internos:**
* `Comparator`: Tipo de funÃ§Ã£o que compara dois elementos e atualiza `SortMetrics`.

**MÃ©todos PÃºblicos:**
* `mergeSort(...)`
  * Algoritmo de ordenaÃ§Ã£o estÃ¡vel.
  * Complexidade garantida de *O(n log n)*.
  * Adequado para anÃ¡lises comparativas previsÃ­veis.

* `quickSort(...)`
  * Algoritmo eficiente na mÃ©dia (*O(n log n)*), porÃ©m com pior caso *O(nÂ²)*.
  * Ãštil para estudos comparativos de desempenho empÃ­rico.

**MÃ©todos Privados (Auxiliares):**
* `merge(...)`: Combina subvetores ordenados no Merge Sort.
* `mergeSortRec(...)`: ImplementaÃ§Ã£o recursiva do Merge Sort.
* `partition(...)`: FunÃ§Ã£o de particionamento do Quick Sort.
* `quickSortRec(...)`: ImplementaÃ§Ã£o recursiva do Quick Sort.

---
### ğŸ“– Leitura de Texto e AnÃ¡lise LÃ©xico-SintÃ¡tica

---
### `class TextReader`
A classe `TextReader` encapsula o acesso ao arquivo de texto, abstraindo operaÃ§Ãµes de entrada e permitindo que o analisador se concentre exclusivamente na lÃ³gica de processamento linguÃ­stico.
Essa classe garante uma leitura controlada e segura do texto, servindo como base para toda a anÃ¡lise subsequente.

**Responsabilidades principais**
* Gerenciar a abertura e fechamento do arquivo de entrada.
* Permitir leitura sequencial linha a linha.
* Manter controle explÃ­cito do nÃºmero da linha atual.

**Atributos:**
* `ifstream file`: fluxo de leitura do arquivo.
* `int currentLine`: contador da linha atual, iniciado em zero.

**MÃ©todos:**
* `TextReader(const string&)`: abre o arquivo especificado.
* `~TextReader()`: fecha o arquivo se estiver aberto.
* `isOpen()`: verifica se o arquivo foi aberto corretamente.
* `hasNextLine()`: indica se ainda existem linhas a serem lidas.
* `nextLine()`: retorna a prÃ³xima linha do arquivo e incrementa o contador.
* `getCurrentLine()`: retorna o nÃºmero da linha atual.

---
#### `class Analyzer`
A classe `Analyzer` concentra toda a lÃ³gica de anÃ¡lise lÃ©xica, sintÃ¡tica e estatÃ­stica do texto. Sua arquitetura foi projetada para integrar mÃºltiplas estruturas de dados clÃ¡ssicas (listas, filas, pilhas, tabelas hash) em um fluxo Ãºnico e coerente, permitindo tanto anÃ¡lise funcional do texto quanto experimentaÃ§Ã£o empÃ­rica de algoritmos.

**Atributos**
* `HashTable<Token> tokens`: todos os tokens do documento.
* `LinkedList<HashTable<Token>> sentenceTokens`: tokens agrupados por sentenÃ§a.
* `HashTable<Expression> allExpressions`: expressÃµes detectadas globalmente.
* `LinkedList<HashTable<Expression>> paragraphExpressions`: expressÃµes por parÃ¡grafo.
* `Queue<Sentence> sentences`: sentenÃ§as detectadas.
* `Queue<Paragraph> paragraphs`: parÃ¡grafos detectados.
* `Queue<Stack<char>> punctuationBalance`: verificaÃ§Ã£o de balanceamento de pontuaÃ§Ã£o por parÃ¡grafo.
* `LinkedList<string> stopWords`: palavras irrelevantes.
* `LinkedList<string> expressions`: expressÃµes compostas prÃ©-definidas.
* `LinkedList<string> abbreviations`: abreviaÃ§Ãµes conhecidas.
* `IntIntMap lengthDist`: distribuiÃ§Ã£o do comprimento das palavras.
* `LinkedList<SortMetrics> benchmarkMetrics`: mÃ©tricas de ordenaÃ§Ã£o.
* `LinkedList<int> benchmarkTests`: tamanhos de entrada usados nos testes.

**MÃ©todos Privados:**
* `loadStopWords(const string &filename)`: LÃª um arquivo contendo palavras irrelevantes (*stop words*) e as armazena em uma lista encadeada apÃ³s normalizaÃ§Ã£o.
  Essas palavras sÃ£o posteriormente ignoradas na anÃ¡lise estatÃ­stica dos tokens.
* `loadExpressions(const string &filename)`: Carrega expressÃµes compostas previamente definidas, armazenando-as em uma lista encadeada para posterior detecÃ§Ã£o no texto.
* `loadAbbreviations()`: Registra um conjunto fixo de abreviaÃ§Ãµes comuns, utilizado para evitar a segmentaÃ§Ã£o incorreta de sentenÃ§as ao encontrar pontos finais (seria interessante implementar um arquivo prÃ³prio de abreviaÃ§Ãµes caso continuasse o desenvolvimento do sistema).
* `isSentenceEnd(char c)`: Verifica se um caractere representa um delimitador de fim de sentenÃ§a (`.`, `!`, `?`).
* `isWordChar(unsigned char c)`: Determina se um caractere pode fazer parte de uma palavra, incluindo letras ASCII e caracteres UTF-8 acentuados.
* `isStopWord(string &word)`: Verifica se uma palavra pertence Ã  lista de *stop words*, indicando se deve ser desconsiderada na contagem estatÃ­stica.
* `checkExpressions(const string &line, int lineNumber, HashTable<Expression> &currentExpressions)`: Procura todas as expressÃµes conhecidas dentro de uma linha normalizada, registrando suas ocorrÃªncias tanto no contexto global quanto no contexto do parÃ¡grafo atual.
* `utf8ToAscii(unsigned char lead, unsigned char next)`: Converte caracteres UTF-8 acentuados para seus equivalentes ASCII, permitindo tratamento uniforme das palavras.
* `normalizeWord(const string &word)`: Normaliza uma palavra individual, removendo acentos, convertendo para minÃºsculas e preservando hÃ­fens em palavras compostas.
* `normalizeLine(const string &line)`: Aplica a normalizaÃ§Ã£o a uma linha inteira, mantendo apenas caracteres relevantes para a anÃ¡lise textual.
* `finalizeSentenceIfPending(...)`: Finaliza uma sentenÃ§a que ainda nÃ£o foi encerrada explicitamente por um delimitador. Calcula mÃ©tricas da sentenÃ§a (como mÃ©dia do comprimento das palavras), cria o objeto `Sentence` correspondente e armazena os tokens associados.
* `isDotInNumber(const string &line, size_t i)`: Verifica se um ponto (`.`) estÃ¡ entre dÃ­gitos numÃ©ricos, caracterizando um nÃºmero decimal e nÃ£o o fim de uma sentenÃ§a.
* `isAbbreviation(const string &word)`: Verifica se a Ãºltima palavra processada corresponde a uma abreviaÃ§Ã£o conhecida, evitando a segmentaÃ§Ã£o indevida de sentenÃ§as.
* `isOpening(char c)`: Identifica sÃ­mbolos de abertura de pontuaÃ§Ã£o, como parÃªnteses, colchetes, chaves e aspas.
* `isClosing(char c)`: Identifica sÃ­mbolos de fechamento de pontuaÃ§Ã£o.

* `matches(char open, char close)`: Verifica se um sÃ­mbolo de abertura corresponde corretamente a um sÃ­mbolo de fechamento.
* `generateLengthDistribution()`: Gera a distribuiÃ§Ã£o do comprimento das palavras, agregando as frequÃªncias em um mapa do tipo `(comprimento â†’ ocorrÃªncia)` e ordenando os resultados por comprimento.
* `runSortingBenchmarks()`: Executa testes de desempenho dos algoritmos de ordenaÃ§Ã£o implementados (Merge Sort e Quick Sort), utilizando diferentes tamanhos de entrada e critÃ©rios de comparaÃ§Ã£o. As mÃ©tricas de comparaÃ§Ãµes, trocas e tempo de execuÃ§Ã£o sÃ£o armazenadas para anÃ¡lise posterior.

**MÃ©todos PÃºblicos**
* `Analyzer(const string &stopWordsFilename, const string &expressionsFilename)`: Inicializa o analisador carregando os arquivos de *stop words* e de expressÃµes compostas, alÃ©m de registrar abreviaÃ§Ãµes conhecidas. Esse prÃ©-processamento garante que os critÃ©rios linguÃ­sticos estejam disponÃ­veis antes do inÃ­cio da anÃ¡lise do texto.
* `analyze(TextReader &reader)`: Executa a anÃ¡lise completa do texto fornecido pelo `TextReader`, populando todas as estruturas internas com os resultados lÃ©xicos, sintÃ¡ticos e estatÃ­sticos.
* `cloneArray(Token* src, int n)`: Cria uma cÃ³pia de um vetor de tokens, utilizada nos testes de ordenaÃ§Ã£o para garantir condiÃ§Ãµes iniciais idÃªnticas.
* `getTokens()`: Retorna a tabela hash com todos os tokens do documento.
* `getSentenceTokens()`: Retorna os tokens agrupados por sentenÃ§a.
* `getAllExpressions()`: Retorna todas as expressÃµes detectadas no texto.
* `getParagraphExpressions()`: Retorna as expressÃµes agrupadas por parÃ¡grafo.
* `getSentences()`: Retorna a fila de sentenÃ§as identificadas.
* `getParagraphs()`: Retorna a fila de parÃ¡grafos identificados.
* `getPunctuationBalance()`: Retorna o balanceamento de pontuaÃ§Ã£o por parÃ¡grafo.
* `getLengthDist()`: Retorna a distribuiÃ§Ã£o do comprimento das palavras.
* `getBenchmarkMetrics()`: Retorna as mÃ©tricas dos testes de ordenaÃ§Ã£o.
* `getBenchmarkTests()`: Retorna os tamanhos de entrada utilizados nos benchmarks.

**Fluxo Geral de ExecuÃ§Ã£o (`analyze`)**
1. Leitura sequencial do texto.
2. NormalizaÃ§Ã£o e anÃ¡lise caractere a caractere.
3. IdentificaÃ§Ã£o de tokens, sentenÃ§as e parÃ¡grafos.
4. Coleta de mÃ©tricas linguÃ­sticas e estatÃ­sticas.
5. GeraÃ§Ã£o de distribuiÃ§Ãµes.
6. ExecuÃ§Ã£o de benchmarks de ordenaÃ§Ã£o.

---
### ğŸ“œ SaÃ­das do Algoritmo

---
#### `class Report`
A classe `Report` Ã© responsÃ¡vel pela geraÃ§Ã£o dos relatÃ³rios textuais e arquivos CSV a partir dos dados processados pela classe `Analyzer`. Ela centraliza toda a lÃ³gica de formataÃ§Ã£o, ordenaÃ§Ã£o e apresentaÃ§Ã£o dos resultados, separando claramente a anÃ¡lise dos dados da apresentaÃ§Ã£o dos resultados, o que melhora a modularidade e a organizaÃ§Ã£o do sistema.

**Atributos**
* `Analyzer &analyzer`: ReferÃªncia do Analyzer que contÃ©m os dados processados.
* `ostream &out`: Fluxo de saÃ­da usado para redigir o `output.txt`.

**MÃ©todos Privados**
* `printLine(char c, int n)`: Imprime uma linha composta por um caractere repetido, utilizada para separar visualmente seÃ§Ãµes do relatÃ³rio.
* `printTitle(const string &title)`: Imprime um tÃ­tulo centralizado, delimitado por linhas decorativas, identificando seÃ§Ãµes principais do relatÃ³rio.
* `printPartialResult()`: Gera o relatÃ³rio parcial, apresentando os resultados organizados por parÃ¡grafo e sentenÃ§a, incluindo tokens, expressÃµes e balanceamento de pontuaÃ§Ã£o.
* `printParagraphPartial(...)`: Produz o relatÃ³rio detalhado de um Ãºnico parÃ¡grafo, exibindo:
  * Tokens ordenados por sentenÃ§a;
  * Palavras mais frequentes no parÃ¡grafo;
  * ExpressÃµes detectadas;
  * VerificaÃ§Ã£o do balanceamento de sÃ­mbolos de pontuaÃ§Ã£o;
  * Metadados do parÃ¡grafo.
* `printFullResult()`: Gera o relatÃ³rio global consolidado, incluindo:
  * Lista completa de tokens do documento;
  * Tamanho do vocabulÃ¡rio distinto;
  * Palavras mais e menos frequentes;
  * EstatÃ­sticas de sentenÃ§as e parÃ¡grafos;
  * ExpressÃµes detectadas;
  * MÃ©tricas completas de desempenho dos algoritmos de ordenaÃ§Ã£o.
* `printSortedTokenTable(HashTable<Token> &hash, int paragraph, int sentence)`: Exibe uma tabela de tokens ordenados alfabeticamente para uma sentenÃ§a especÃ­fica.
* `printFrequentlyUsedWords(HashTable<Token> &hash)`: Lista palavras que aparecem trÃªs ou mais vezes dentro de um parÃ¡grafo, destacando termos recorrentes.
* `printSortedExpressionTable(HashTable<Expression> &hash)`: Apresenta as expressÃµes detectadas, ordenadas alfabeticamente, juntamente com suas frequÃªncias e linhas de ocorrÃªncia.
* `printMostLeastFrequentWords(HashTable<Token> &hash, int X)`: Exibe as *X* palavras mais frequentes e as *X* menos frequentes do documento.
* `printSentenceStats()`: Apresenta estatÃ­sticas por sentenÃ§a, incluindo:
  * Quantidade de palavras com e sem *stop words*;
  * Comprimento mÃ©dio das palavras;
  * Comprimento mÃ©dio das sentenÃ§as no documento.
* `printParagraphStats()`: Exibe informaÃ§Ãµes estruturais dos parÃ¡grafos, como linha inicial e nÃºmero total de sentenÃ§as.
* `printLengthDist()`: Mostra a distribuiÃ§Ã£o do comprimento das palavras, baseada nos dados agregados pelo analisador.

* `printFullResultSortMetrics(HashTable<Token> &hash)`: Executa novamente os algoritmos de ordenaÃ§Ã£o sobre o conjunto completo de tokens e exibe mÃ©tricas de:
  * ComparaÃ§Ãµes;
  * Trocas;
  * Tempo de execuÃ§Ã£o.
* `exportSortMetricsCSV()`: Exporta as mÃ©tricas de desempenho dos algoritmos de ordenaÃ§Ã£o para um arquivo CSV, utilizado posteriormente na anÃ¡lise grÃ¡fica.
* `exportLengthDist()`: Exporta a distribuiÃ§Ã£o do comprimento das palavras para um arquivo CSV.

**MÃ©todos PÃºblicos**
* `Report(Analyzer &a, ostream &output)`: Inicializa o relatÃ³rio com uma referÃªncia constante ao objeto `Analyzer`, que contÃ©m todos os dados processados, e um fluxo de saÃ­da (`ostream`) onde o relatÃ³rio serÃ¡ escrito.
* `generate()`: MÃ©todo principal da classe, responsÃ¡vel por:
  * Gerar o relatÃ³rio parcial;
  * Gerar o relatÃ³rio completo;
  * Exportar os arquivos CSV auxiliares.

---
#### FunÃ§Ã£o ``main``
A funÃ§Ã£o `main` Ã© o ponto de entrada do sistema, sendo responsÃ¡vel por coordenar a execuÃ§Ã£o das principais classes do projeto.
Ela Ã© responsÃ¡vel por: Validar os argumentos de linha de comando (nome do arquivo de entrada); Abrir arquivo de entrada; Instanciar `Analyzer` com os arquivos de configuraÃ§Ã£o; Executar a anÃ¡lise textual; Criar objeto `Report` e gerar relatÃ³rio final; Encerrar o programa com mensagem de sucesso.

---
### ğŸ“Š Scripts em Python
Os scripts em Python sÃ£o utilizados para pÃ³s-processamento e visualizaÃ§Ã£o grÃ¡fica dos dados gerados pelo sistema em C++. Essa abordagem permite separar a anÃ¡lise algorÃ­tmica da visualizaÃ§Ã£o, facilitando experimentaÃ§Ã£o e reutilizaÃ§Ã£o dos dados.

---
#### `plot_utils.py`
MÃ³dulo utilitÃ¡rio que centraliza a lÃ³gica de salvamento de figuras do Matplotlib.
**FunÃ§Ã£o:**
* `save_figure(output_dir, filename)`: Garante a existÃªncia do diretÃ³rio de saÃ­da, salva a figura com alta resoluÃ§Ã£o e libera recursos de memÃ³ria.

---
#### `plot_length_dist.py`
Script responsÃ¡vel por gerar o grÃ¡fico de "distribuiÃ§Ã£o do comprimento das palavras".
**Funcionalidades:**
* Leitura do arquivo `length_dist.csv`;
* OrdenaÃ§Ã£o dos dados por comprimento da palavra;
* Plotagem de um grÃ¡fico de barras representando frequÃªncia por tamanho;
* Salvamento automÃ¡tico da figura no diretÃ³rio de saÃ­da.

Esse grÃ¡fico permite analisar padrÃµes linguÃ­sticos relacionados ao tamanho mÃ©dio das palavras no texto.

---
#### `plot_sort_metrics.py`
Script responsÃ¡vel por gerar grÃ¡ficos de **desempenho dos algoritmos de ordenaÃ§Ã£o**.
**Funcionalidades:**
* Leitura do arquivo `sort_metrics.csv`;
* Agrupamento dos dados por algoritmo e critÃ©rio de ordenaÃ§Ã£o;
* GeraÃ§Ã£o de grÃ¡ficos de linha para:
  * Tempo de execuÃ§Ã£o;
  * NÃºmero de comparaÃ§Ãµes;
  * NÃºmero de trocas;
* ComparaÃ§Ã£o visual entre Merge Sort e Quick Sort.

Esses grÃ¡ficos permitem avaliar empiricamente o comportamento dos algoritmos em diferentes tamanhos de entrada.

---
## ğŸ§® Resultados
Essa seÃ§Ã£o apresenta os principais resultados obtidos a partir da execuÃ§Ã£o do Analisador LÃ©xico-SintÃ¡tico, considerando o arquivo de entrada de exemplo ``data/Semana_Machado_Assis.txt``.
### Exemplo de saÃ­da do analisador
O arquivo `output/output.txt` contÃ©m o relatÃ³rio textual completo gerado pelo sistema. Segue uma parte da saÃ­da para o texto ``data/Semana_Machado_Assis.txt``:
```
======================================================================================================================================================
=>                                                                ### START PROCESS ###

======================================================================================================================================================
======================================================================================================================================================
=>                                                                ### PARTIAL RESULT ###

======================================================================================================================================================
______________________________________________________________________________________________________________________________________________________
WORD                     FREQUENCY      PARAGRAPH      SENTENCE       LINE           POSITIONS
------------------------------------------------------------------------------------------------------------------------------------------------------
assis                    1              1              1              1              9 
completa                 1              1              1              1              5 
machado                  1              1              1              1              7 
obra                     1              1              1              1              4 
semana                   1              1              1              1              2 
texto-fonte              1              1              1              1              3 
______________________________________________________________________________________________________________________________________________________
=> Number of words with stop words: 9                                                           => Number of words without stop words: 6
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
=> Balanced symbols: YES
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
=> Beginning paragraph in line: 1  Number of sentences: 1
______________________________________________________________________________________________________________________________________________________

...

______________________________________________________________________________________________________________________________________________________
DISTINCT VOCABULARY SIZE: 24721 WORDS
------------------------------------------------------------------------------------------------------------------------------------------------------
______________________________________________________________________________________________________________________________________________________
TOP 10 MOST FREQUENT WORDS
WORD                FREQUENCY   
------------------------------------------------------------------------------------------------------------------------------------------------------
ainda               629
tudo                563
pode                548
outro               475
homem               471
todos               455
assim               454
outra               437
aqui                421
outros              419
______________________________________________________________________________________________________________________________________________________
______________________________________________________________________________________________________________________________________________________
TOP 10 LEAST FREQUENT WORDS
WORD                FREQUENCY   
------------------------------------------------------------------------------------------------------------------------------------------------------
repetirei           1
desazo              1
refuta              1
esfriaras           1
agasalhar           1
di-lo               1
personalidades      1
apaga               1
restante            1
birmingham          1
______________________________________________________________________________________________________________________________________________________

...
```
**Exemplo de saÃ­da de output.txt**

### DistribuiÃ§Ã£o por comprimento das palavras
A distribuiÃ§Ã£o do comprimento das palavras foi obtida a partir do arquivo ``length_dist.csv`` e visualizada no grÃ¡fico apresentado abaixo, armazenado no diretÃ³rio `assets`.
![DistribuiÃ§Ã£o por comprimento](assets/length_distribution.png)
> **Figura 1 - DistribuiÃ§Ã£o por comprimento para `data/Semana_Machado_Assis.txt`.**

Ã‰ notÃ¡vel que a maioria das palavras do texto estÃ£o com comprimento em torno de 5 caractÃ©res, o que Ã© aceitÃ¡vel para lingua portuguesa.
### Desempenho dos algoritmos de ordenaÃ§Ã£o
O desempenho dos algoritmos MergeSort e QuickSort foi avaliado considerando dois critÃ©rios de ordenaÃ§Ã£o:
* Ordem alfabÃ©tica;
* Ordem por frequÃªncia.
As mÃ©tricas analisadas foram:
* Tempo de execuÃ§Ã£o (em segundos);
* NÃºmero de comparaÃ§Ãµes;
* NÃºmero de trocas (swaps).
Os resultados estÃ£o novamente no diretÃ³rio `assets`.
#### Tempo de execuÃ§Ã£o
![Performance tempo](assets/sort_performance_time.png)
> **Figura 2 - GrÃ¡fico de performance em relaÃ§Ã£o ao tempo.**

Percebe-se que o tempo do QuickSort por frequÃªncia tem um aumento brusco para $n\approx 25000$, provavelmente resultado de pivos ruins de partiÃ§Ã£o, enquanto que os outros algoritmos de ordenaÃ§Ã£o ficam prÃ³ximos entre sÃ­ no valor de 0.5 segundos.
#### ComparaÃ§Ãµes e trocas
![Performance comparaÃ§Ãµes](assets/sort_performance_comparisons.png)
> **Figura 3 - GrÃ¡fico de performance em relaÃ§Ã£o Ã s comparaÃ§Ãµes.**

Nas comparaÃ§Ãµes, nota-se que o QuickSort por frequÃªncia novamente tem um estouro de comparaÃ§Ãµes em relaÃ§Ã£o aos outros algoritmos tendo mais de 90 milhÃµes de comparaÃ§Ãµes. Isso pode ser devido a grande quantidade de chaves com mesmo valor (palavras com a mesma frequÃªncia), o que causa esse comportamento descontrolado do QuickSort.
Enquanto que os outros algorimtos ficaram todos com menos de meio milhÃ£o de comparaÃ§Ãµes.
![Performance trocas](assets/sort_performance_swaps.png)
> **Figura 4 - GrÃ¡fico de performance em relaÃ§Ã£o Ã s trocas.**

No caso de trocas, ocorre o efeito contrÃ¡rio, pois devido a vÃ¡rias palavras terem a mesma frequÃªncia, o algoritmo do QuickSort consegue melhor desempenho graÃ§as a sua adaptabilidade (capacidade de melhorar o desempenho caso a estrutura jÃ¡ esteja parcialmente ordenada).
O QuickSort por ordem alfabÃ©tica tambÃ©m se beneficiou da adaptabilidade.
### ComparaÃ§Ã£o com anÃ¡lise assintÃ³tica
A anÃ¡lise de desempenho dos algoritmos de ordenaÃ§Ã£o considerou tanto os tempos de execuÃ§Ã£o observados experimentalmente quanto o comportamento assintÃ³tico esperado para cada mÃ©todo. Conforme definido no arquivo de `prÃ¡tica.pdf`, o custo teÃ³rico pode ser modelado por:
$$T(n)=c_1\times nlog(n)+c_2\times n+O(1)$$
onde as constantes $c_1$ e $c_2$ dependem das operaÃ§Ãµes elementares realizadas, como comparaÃ§Ãµes e trocas, alÃ©m das caracterÃ­sticas do hardware utilizado.
A fim de estimar o valor dessas constantes para comparar com o valores obtidos da execuÃ§Ã£o, aproximou-se a formula para:
$$T(n)\approx c_1\times log_2(n)$$
pois $O(1)$ Ã© desprezÃ­vel para grandes valore de $n$ e as comparaÃ§Ãµes sÃ£o o maior custo. Ao utilizar o ponto experimental de $n\approx 25000$, obtÃ©m-se uma estimativa para $c_1$:
MergeSort (ordem alfabÃ©tica, $n=24721$)
* $n=24721$
* $T_{real} = 0.504115 \; s$ 
* $log_2(24721)\approx 14.6$
$$n\;log_2\;n\approx 24721\times 14.6 \approx 360927$$
Logo:
$$c_1 \approx \frac{T{real}}{n\; log\; n} = \frac{0.504115}{360927} \approx \boxed{1.4\times 10^{-6}}$$
Usando esse $c_1$ para o MergeSort alfabÃ©tico:

|$n$|$n\; log\; n$|$T_{teÃ³rico}(s)$|$T_{real}(s)$|
|-|-|-|-|
|$1000$|$9966$|$0.014$|$0.016$|
|$5000$|$61440$|$0.086$|$0.096$|
|$10000$|$132880$|$0.186$|$0.197$|
|$24721$|$360927$|$0.505$|$0.504$|
> **Tabela 2 - AproximaÃ§Ã£o do tempo assintÃ³tico do MergeSort alfabÃ©tico.**

A partir dessa tabela, nota-se que os valores de $T_{teÃ³rico}(s)$ se aproximam consideravelmente do tempo $T_{real}(s)$.

JÃ¡ para o QuickSort por frequÃªncia, encontra essa relaÃ§Ã£o:
|$n$|ComparaÃ§Ãµes|
|-|-|
|$5000$|$3683894$|
|$10000$|$14768827$|
|$24721$|$91232211$|
> **Tabela 3 - RelaÃ§Ã£o do tamanho $n$ para o nÃºmero de comparaÃ§Ãµes do QuickSort por frequÃªncia.**

E considerando o crescimento:
* $5k \to 10k \approx 4.0$
* $10k \to 24k \approx 6.177$

Encontra-se um comportamento mais prÃ³ximo de $n^2$ do que $n\;log\;n$. Como havia mencionado anteriormente, isso pode ser causado por muitas frequÃªncias iguais e o QuickSort implementado nÃ£o usa mediana de trÃªs, causando um $T(n)\approx c\times n^2$, mesmo que o tempo de execuÃ§Ã£o seja aceitÃ¡vel, o nÃºmero de comparaÃ§Ãµes explode, confirmando o pior caso teÃ³rico.

## ğŸ ConclusÃ£o
A execuÃ§Ã£o do Analisador LÃ©xico-SintÃ¡tico sobre o arquivo `data/Semana_Machado_Assis.txt` permitiu avaliar, de forma prÃ¡tica e quantitativa, tanto a qualidade da anÃ¡lise textual quanto o desempenho dos algoritmos de ordenaÃ§Ã£o aplicados ao processamento dos dados lÃ©xicos. O relatÃ³rio gerado em `output/output.txt` evidencia a correta extraÃ§Ã£o de informaÃ§Ãµes como frequÃªncia de palavras, distribuiÃ§Ã£o por sentenÃ§as e parÃ¡grafos, vocabulÃ¡rio distinto e verificaÃ§Ã£o de balanceamento de sÃ­mbolos, confirmando a eficÃ¡cia das estruturas de dados desenvolvidas.
O elevado tamanho do vocabulÃ¡rio distinto (24 721 palavras) confirma a riqueza lexical do texto analisado, caracterÃ­stica esperada de uma obra literÃ¡ria extensa. A distribuiÃ§Ã£o por comprimento das palavras, obtida a partir de `length_dist.csv`, apresentou concentraÃ§Ã£o em torno de palavras com aproximadamente cinco caracteres, com decaimento gradual para comprimentos maiores, comportamento coerente com a lÃ­ngua portuguesa e indicativo do correto funcionamento do processo de tokenizaÃ§Ã£o e agrupamento estatÃ­stico.
Quanto ao desempenho dos algoritmos de ordenaÃ§Ã£o, os resultados experimentais foram analisados Ã  luz do modelo assintÃ³tico, permitindo comparar os tempos medidos com os tempos teÃ³ricos estimados. Para o MergeSort, observou-se forte aderÃªncia ao comportamento ($O(n \log n)$) em todos os cenÃ¡rios, com crescimento estÃ¡vel do tempo de execuÃ§Ã£o e boa aproximaÃ§Ã£o entre valores empÃ­ricos e teÃ³ricos, evidenciando sua previsibilidade e robustez.
Em contrapartida, o QuickSort apresentou desempenho dependente do critÃ©rio de ordenaÃ§Ã£o. Na ordenaÃ§Ã£o alfabÃ©tica, o comportamento manteve-se prÃ³ximo do caso mÃ©dio ($O(n \log n)$); entretanto, na ordenaÃ§Ã£o por frequÃªncia, verificou-se aumento significativo no nÃºmero de comparaÃ§Ãµes para grandes valores de ($n$), aproximando-se do pior caso ($O(n^2)$). Esse efeito estÃ¡ associado Ã  alta incidÃªncia de frequÃªncias repetidas e Ã  ausÃªncia da estratÃ©gia de mediana de trÃªs, resultando em partiÃ§Ãµes desbalanceadas.
Dessa forma, os resultados prÃ¡ticos corroboram a anÃ¡lise assintÃ³tica clÃ¡ssica: o MergeSort mostrou-se mais adequado ao contexto da anÃ¡lise textual, caracterizado por grandes volumes de dados e muitas chaves repetidas, enquanto o QuickSort apresentou melhor desempenho em situaÃ§Ãµes favorÃ¡veis, porÃ©m com maior sensibilidade Ã  distribuiÃ§Ã£o dos dados. O trabalho, portanto, atingiu seus objetivos ao integrar anÃ¡lise lÃ©xico-sintÃ¡tica, avaliaÃ§Ã£o experimental de algoritmos e comparaÃ§Ã£o com modelos teÃ³ricos de tempo de execuÃ§Ã£o.
## ğŸ”§ ConfiguraÃ§Ã£o do Ambiente

Para garantir a correta compilaÃ§Ã£o e execuÃ§Ã£o do Analisador LÃ©xico-SintÃ¡tico, Ã© necessÃ¡rio que o ambiente de desenvolvimento esteja configurado conforme as especificaÃ§Ãµes a seguir.

* **Sistema Operacional**:
  Linux Ubuntu 22.04 ou 24.04 LTS (recomendado).
  O projeto tambÃ©m pode ser compilado em Windows, desde que o compilador e as ferramentas estejam corretamente configurados.

* **Compilador**:
  GCC versÃ£o 13 ou superior, com suporte ao padrÃ£o **C++17**.

Para verificar a versÃ£o instalada do compilador, utilize:

```bash
g++ --version
```

Caso seja necessÃ¡rio instalar ou atualizar o compilador e as ferramentas essenciais de build no Ubuntu, execute:

```bash
sudo apt update
sudo apt install build-essential g++
```

* **Bibliotecas**:
  O projeto utiliza exclusivamente a biblioteca padrÃ£o da linguagem C++. NÃ£o hÃ¡ dependÃªncias externas de terceiros.

* **Python (opcional)**:
  Python 3 e o `venv` sÃ£o utilizados apenas para a geraÃ§Ã£o dos grÃ¡ficos de anÃ¡lise de desempenho e distribuiÃ§Ã£o por comprimento, atravÃ©s dos scripts localizados no diretÃ³rio `utils/`.

---

## ğŸ’» Como Compilar e Executar

O projeto utiliza um **Makefile** para padronizar os processos de compilaÃ§Ã£o, execuÃ§Ã£o e geraÃ§Ã£o de grÃ¡ficos, garantindo reprodutibilidade dos experimentos.
### Clone o RepositÃ³rio
``` bash
git clone https://github.com/JV-NC/LexicalSyntacticAnalyzer.git
```

### Estrutura de Entrada

* **Arquivo de entrada textual**:
  Por padrÃ£o, o sistema utiliza o arquivo:

  ```
  data/DomCasmurro.txt
  ```

  O arquivo de entrada pode ser alterado via variÃ¡vel `INPUT` no momento da execuÃ§Ã£o, sem necessidade de modificar o cÃ³digo-fonte.

Exemplo:

```bash
make run INPUT=data/Semana_Machado_Assis.txt
```

---

### CompilaÃ§Ã£o

Para compilar o projeto, navegue atÃ© o diretÃ³rio raiz e execute:

```bash
make clean
make
```

* `make clean`: remove arquivos objeto (`.o`), diretÃ³rios de build e saÃ­das anteriores, garantindo uma compilaÃ§Ã£o limpa.
* `make`: compila todos os arquivos `.cpp` presentes em `src/` e gera o executÃ¡vel no diretÃ³rio `bin/`.

---

### ExecuÃ§Ã£o

ApÃ³s a compilaÃ§Ã£o, o analisador pode ser executado com:

```bash
make run
```

Esse comando irÃ¡:

* Executar o analisador lÃ©xico-sintÃ¡tico sobre o arquivo definido em `INPUT`;
* Processar o texto, realizando tokenizaÃ§Ã£o, anÃ¡lise sintÃ¡tica bÃ¡sica e coleta de estatÃ­sticas;
* Gerar o relatÃ³rio textual no diretÃ³rio `output/`, incluindo o arquivo `output.txt`.

---

### GeraÃ§Ã£o dos GrÃ¡ficos

Para gerar os grÃ¡ficos de **distribuiÃ§Ã£o por comprimento** e **desempenho dos algoritmos de ordenaÃ§Ã£o**, execute:

```bash
make plots
```

Esse comando:

* Executa o analisador (caso ainda nÃ£o tenha sido executado);
* Utiliza os scripts Python em `utils/` para processar os arquivos CSV gerados;
* Salva os grÃ¡ficos no diretÃ³rio `output/`.

---

### ExecuÃ§Ã£o Completa

Para realizar todo o processo automaticamente (limpeza, compilaÃ§Ã£o, execuÃ§Ã£o e geraÃ§Ã£o de grÃ¡ficos), utilize:

```bash
make full
```

Esse fluxo garante a completa reprodutibilidade dos resultados apresentados neste relatÃ³rio.

---
## ğŸ–¥ï¸ Hardware Utilizado

Os experimentos apresentados na SeÃ§Ã£o de Resultados foram executados em um ambiente controlado, utilizando um Ãºnico sistema computacional, com o objetivo de garantir consistÃªncia e reprodutibilidade nas mediÃ§Ãµes de desempenho dos algoritmos analisados. Todas as mediÃ§Ãµes de tempo, nÃºmero de comparaÃ§Ãµes e nÃºmero de trocas foram obtidas a partir da execuÃ§Ã£o local do analisador lÃ©xico-sintÃ¡tico, sem concorrÃªncia de outros processos computacionalmente intensivos, buscando minimizar interferÃªncias externas nos resultados experimentais.

|Componente|EspecificaÃ§Ã£o|
|-|-|
|Processador|AMD Ryzen 7 5700G|
|Arquitetura|x86_64|
|FrequÃªncia Base|3.80GHz|
|MemÃ³ria RAM|16GB DDR4 3200MHz|
|Sistema Operacional|Windows 10 22H2|
|Compilador|GCC 6.3.0|
> **Tabela 4 - EspecificaÃ§Ãµes do hardware utilizado nos experimentos.**

----
## ğŸ‘¤ Autoria

<div align="center">
  <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=22&pause=1000&color=6366F1&center=true&vCenter=true&width=400&lines=Autor+do+Projeto;Aproveitamento+Estudos" alt="Typing SVG" />
</div>

### ğŸš€ Autor

<table align="center">
  <tr>
    <td align="center">
      <a href="https://github.com/JV-NC">
        <img src="https://github.com/JV-NC.png" width="120px;" alt="JoÃ£o Vitor Neves"/><br>
        <sub><b>JoÃ£o Vitor Neves</b></sub>
      </a><br><br>
      <a href="https://github.com/joaovitor3105" title="GitHub">
        <img src="https://img.shields.io/github/followers/JV-NC?label=Seguidores&style=social&logo=github" alt="GitHub - JoÃ£o Vitor">
      </a>
    </td>
  </tr>
</table>

### ğŸ“Š EstatÃ­sticas do RepositÃ³rio

<div align="center">
  <img src="https://img.shields.io/github/commit-activity/t/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=git&label=Total%20Commits" alt="Total de commits">
  <img src="https://img.shields.io/github/languages/top/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=cplusplus" alt="Linguagem principal">
  <img src="https://img.shields.io/github/issues-pr-closed/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=github&label=PRs%20Fechados" alt="PRs Fechados">
  <img src="https://img.shields.io/github/license/JV-NC/LexicalSyntacticAnalyzer?style=for-the-badge&logo=open-source-initiative&label=LicenÃ§a" alt="LicenÃ§a do projeto">
</div>

---
## ğŸ“š ReferÃªncias

Esta seÃ§Ã£o reÃºne os principais materiais teÃ³ricos, tÃ©cnicos e documentais utilizados como base para o desenvolvimento do Analisador LÃ©xico-SintÃ¡tico, bem como para a anÃ¡lise de desempenho dos algoritmos de ordenaÃ§Ã£o e a interpretaÃ§Ã£o dos resultados experimentais apresentados neste trabalho.
